{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Example: Time Series Model - Bouncing MNIST in NumPyro\n",
        "\n",
        "This example illustrates how to construct an inference program based on the APGS\n",
        "sampler [1] for BMNIST. The details of BMNIST can be found in the sections\n",
        "6.4 and F.3 of the reference. We will use the NumPyro (default) backend for this\n",
        "example.\n",
        "\n",
        "**References**\n",
        "\n",
        "    1. Wu, Hao, et al. Amortized population Gibbs samplers with neural\n",
        "       sufficient statistics. ICML 2020.\n",
        "\n",
        "<img src=\"file://../_static/bmnist.gif\" align=\"center\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/frans/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2024-08-02 22:48:50.858479: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-02 22:48:50.868556: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-02 22:48:50.871600: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-02 22:48:51.446450: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "from functools import partial\n",
        "\n",
        "import coix\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "from jax import random\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib.patches import Rectangle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpyro\n",
        "import numpyro.distributions as dist\n",
        "import optax\n",
        "from optax import cosine_decay_schedule\n",
        "from optax import clip_by_global_norm\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from sim_utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let's load the moving mnist dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# def load_dataset(*, is_training, batch_size):\n",
        "#   ds = tfds.load(\"moving_mnist:1.0.0\", split=\"test\")\n",
        "#   ds = ds.repeat()\n",
        "#   if is_training:\n",
        "#     ds = ds.shuffle(10 * batch_size, seed=0)\n",
        "#     map_fn = lambda x: x[\"image_sequence\"][..., :10, :, :, 0] / 255\n",
        "#   else:\n",
        "#     map_fn = lambda x: x[\"image_sequence\"][..., 0] / 255\n",
        "#   ds = ds.batch(batch_size)\n",
        "#   ds = ds.map(map_fn)\n",
        "#   return iter(tfds.as_numpy(ds))\n",
        "\n",
        "def load_dataset(*, is_training, batch_size):\n",
        "  # ds = np.load(\"worms_train_20k.npy\")\n",
        "  ds = np.load(\"worms_train.npy\")\n",
        "  # make ds a tensor, and batch it\n",
        "  ds = tf.data.Dataset.from_tensor_slices(ds)\n",
        "  ds = ds.repeat()\n",
        "  if is_training:\n",
        "    ds = ds.shuffle(10 * batch_size, seed=0)\n",
        "  ds = ds.batch(batch_size)\n",
        "  # standardize the data between 0 and 1\n",
        "  ds = ds.map(lambda x: x / 0.80999994)\n",
        "  return iter(tfds.as_numpy(ds))\n",
        "\n",
        "def get_digit_mean():\n",
        "  ds, ds_info = tfds.load(\"mnist:3.0.1\", split=\"train\", with_info=True)\n",
        "  ds = tfds.as_numpy(ds.batch(ds_info.splits[\"train\"].num_examples))\n",
        "  digit_mean = next(iter(ds))[\"image\"].squeeze(-1).mean(axis=0)\n",
        "  return digit_mean / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_digit_mean():\n",
        "  ds, ds_info = tfds.load(\"mnist:3.0.1\", split=\"train\", with_info=True)\n",
        "  ds = tfds.as_numpy(ds.batch(ds_info.splits[\"train\"].num_examples))\n",
        "  digit_mean = next(iter(ds))[\"image\"].squeeze(-1).mean(axis=0)\n",
        "  return digit_mean / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define the neural proposals for the Gibbs kernels and the neural\n",
        "decoder for the generative model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vmapped_sim_fn(sim_fn, params):\n",
        "    # print(jax.tree_map(lambda x: x.shape if hasattr(x, 'shape') else None, params))\n",
        "    if params['L'].ndim == 1:\n",
        "        return jax.vmap(sim_fn, in_axes=0, out_axes=0)(params)\n",
        "    else:\n",
        "        return jax.vmap(partial(vmapped_sim_fn, sim_fn), in_axes=0, out_axes=0)(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sim_worms(nworms, batch_size, n_frames):\n",
        "    duration = 0.55\n",
        "    snapshots = 10\n",
        "    kpoints = 6\n",
        "    box_size = 64\n",
        "    with numpyro.plate('batch', batch_size, dim=-2):\n",
        "        with numpyro.plate('nworms', nworms, dim=-1):\n",
        "            # L = numpyro.sample('L', dist.Uniform(30, 45))\n",
        "            L = numpyro.sample('L', dist.Uniform(10, 15))\n",
        "            A = numpyro.sample('A', dist.Normal(1, 0.1))\n",
        "            T = numpyro.sample('T', dist.Normal(0.8, 0.1))\n",
        "            kw = numpyro.sample('kw', dist.Uniform(0, 2 * jnp.pi))\n",
        "            ku = numpyro.sample('ku', dist.Normal(jnp.pi, 1))\n",
        "            \n",
        "            inc = numpyro.sample('inc', dist.Uniform(0, 2 * jnp.pi))\n",
        "            dr = numpyro.sample('dr', dist.Uniform(0.2, 0.8))\n",
        "            phase_1 = numpyro.sample('phase_1', dist.Uniform(0, 2 * jnp.pi))\n",
        "            phase_2 = numpyro.sample('phase_2', dist.Uniform(0, 2 * jnp.pi))\n",
        "            phase_3 = numpyro.sample('phase_3', dist.Normal(0, 0.1))\n",
        "            alpha = numpyro.sample('alpha', dist.Normal(4, 4))\n",
        "\n",
        "            alpha = jnp.abs(alpha + 1.0)\n",
        "            half_box = box_size // 2\n",
        "            x0 = numpyro.sample('x0', dist.Uniform(-1, 1))\n",
        "            y0 = numpyro.sample('y0', dist.Uniform(-1, 1))\n",
        "            x0 = x0 * half_box\n",
        "            y0 = y0 * half_box\n",
        "\n",
        "            params = {'L': L, 'A': A, 'T': T, 'kw': kw, 'ku': ku, 'inc': inc, 'dr': dr, 'phase_1': phase_1, 'phase_2': phase_2, 'phase_3': phase_3, 'alpha': alpha, 'x0': x0, 'y0': y0}\n",
        "\n",
        "            # # L is a tensor. Assert that all values are within the range\n",
        "            # assert jnp.all(L >= 10) and jnp.all(L <= 15), f\"L: {L}\"\n",
        "            # # kw\n",
        "            # assert jnp.all(kw >= 0) and jnp.all(kw <= 2 * jnp.pi), f\"kw: {kw}\"\n",
        "            # # inc\n",
        "            # assert jnp.all(inc >= 0) and jnp.all(inc <= 2 * jnp.pi), f\"inc: {inc}\"\n",
        "            # # dr\n",
        "            # assert jnp.all(dr >= 0.2) and jnp.all(dr <= 0.8), f\"dr: {dr}\"\n",
        "            # # phase_1\n",
        "            # assert jnp.all(phase_1 >= 0) and jnp.all(phase_1 <= 2 * jnp.pi), f\"phase_1: {phase_1}\"\n",
        "            # # phase_2\n",
        "            # assert jnp.all(phase_2 >= 0) and jnp.all(phase_2 <= 2 * jnp.pi), f\"phase_2: {phase_2}\"\n",
        "            # # x0\n",
        "            # assert jnp.all(x0 >= -1) and jnp.all(x0 <= 1), f\"x0: {x0}\"\n",
        "            # # y0\n",
        "            # assert jnp.all(y0 >= -1) and jnp.all(y0 <= 1), f\"y0: {y0}\"\n",
        "\n",
        "            sim_fn = partial(\n",
        "                worm_simulation,\n",
        "                duration=duration,\n",
        "                snapshots=snapshots,\n",
        "                kpoints=kpoints,\n",
        "            )\n",
        "\n",
        "            with numpyro.plate('n_frames', n_frames):\n",
        "                worms = vmapped_sim_fn(sim_fn, params)\n",
        "                \n",
        "                worms = worms + half_box\n",
        "                # subtract mean and divide by standard deviation\n",
        "                worms = (worms - jnp.mean(worms, axis=(-5), keepdims=True)) / jnp.std(worms, axis=(-5), keepdims=True)\n",
        "                numpyro.deterministic('worms', worms)\n",
        "    return worms, x0, y0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def scale_and_translate(image, where, out_size):\n",
        "  translate = abs(image.shape[-1] - out_size) * (where[..., ::-1] + 1) / 2\n",
        "  return jax.image.scale_and_translate(\n",
        "      image,\n",
        "      (out_size, out_size),\n",
        "      (0, 1),\n",
        "      jnp.ones(2),\n",
        "      translate,\n",
        "      method=\"cubic\",\n",
        "      antialias=False,\n",
        "  )\n",
        "\n",
        "\n",
        "def crop_frames(frames, z_where, digit_size=28):\n",
        "  # frames:           time.frame_size.frame_size\n",
        "  # z_where: (digits).time.2\n",
        "  # out:     (digits).time.digit_size.digit_size\n",
        "  if frames.ndim == 2 and z_where.ndim == 1:\n",
        "    return scale_and_translate(frames, z_where, out_size=digit_size)\n",
        "  elif frames.ndim == 3 and z_where.ndim == 2:\n",
        "    in_axes = (0, 0)\n",
        "  elif frames.ndim == 3 and z_where.ndim == 3:\n",
        "    in_axes = (None, 0)\n",
        "  elif frames.ndim == z_where.ndim:\n",
        "    in_axes = (0, 0)\n",
        "  elif frames.ndim > z_where.ndim:\n",
        "    in_axes = (0, None)\n",
        "  else:\n",
        "    in_axes = (None, 0)\n",
        "  return jax.vmap(partial(crop_frames, digit_size=digit_size), in_axes)(\n",
        "      frames, z_where\n",
        "  )\n",
        "\n",
        "\n",
        "def embed_digits(digits, z_where, frame_size=64):\n",
        "  # digits:  (digits).      .digit_size.digit_size\n",
        "  # z_where: (digits).(time).2\n",
        "  # out:     (digits).(time).frame_size.frame_size\n",
        "  if digits.ndim == 2 and z_where.ndim == 1:\n",
        "    return scale_and_translate(digits, z_where, out_size=frame_size)\n",
        "  elif digits.ndim == 2 and z_where.ndim == 2:\n",
        "    in_axes = (None, 0)\n",
        "  elif digits.ndim >= z_where.ndim:\n",
        "    in_axes = (0, 0)\n",
        "  else:\n",
        "    in_axes = (None, 0)\n",
        "  # print(\"in_axes\", in_axes)\n",
        "  # print(digits.shape)\n",
        "  # print(z_where.shape)\n",
        "  return jax.vmap(partial(embed_digits, frame_size=frame_size), in_axes)(\n",
        "      digits, z_where\n",
        "  )\n",
        "\n",
        "  \n",
        "\n",
        "def conv2d(frames, digits):\n",
        "  # frames:          (time).frame_size.frame_size\n",
        "  # digits: (digits).      .digit_size.digit_size\n",
        "  # out:    (digits).(time).conv_size .conv_size\n",
        "  if frames.ndim == 2 and digits.ndim == 2:\n",
        "    return jax.scipy.signal.convolve2d(frames, digits, mode=\"valid\")\n",
        "  elif frames.ndim == digits.ndim:\n",
        "    in_axes = (0, 0)\n",
        "  elif frames.ndim > digits.ndim:\n",
        "    in_axes = (0, None)\n",
        "  else:\n",
        "    in_axes = (None, 0)\n",
        "  return jax.vmap(conv2d, in_axes=in_axes)(frames, digits)\n",
        "\n",
        "\n",
        "# class EncoderWhat(nn.Module):\n",
        "\n",
        "#   @nn.compact\n",
        "#   def __call__(self, digits):\n",
        "#     x = digits.reshape(digits.shape[:-2] + (-1,)) # flatten frame into vector\n",
        "#     x = nn.Dense(400)(x)\n",
        "#     x = nn.relu(x)\n",
        "#     x = nn.Dense(200)(x)\n",
        "#     x = nn.relu(x)\n",
        "\n",
        "#     x = x.sum(-2)  # sum/mean across time\n",
        "#     loc_raw = nn.Dense(10)(x)\n",
        "#     scale_raw = 0.5 * nn.Dense(10)(x)\n",
        "#     return loc_raw, jnp.exp(scale_raw)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderWhat(nn.Module):\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, digits, carry=None):\n",
        "    mappable_dims = digits.shape[:-3]\n",
        "    \n",
        "    hidden_dim = 400\n",
        "\n",
        "    x = digits.reshape(digits.shape[:-2] + (-1,)) # flatten frame into vector\n",
        "    if carry is None:\n",
        "      carry = self.param('carry_init', \n",
        "                         lambda rng, shape: jnp.zeros(shape), \n",
        "                         mappable_dims + (hidden_dim,))\n",
        "    GRU = nn.scan(nn.GRUCell,\n",
        "                  in_axes=-1,\n",
        "                  variable_broadcast='params',\n",
        "                  split_rngs={'params': False}\n",
        "                  )(400)\n",
        "    # print(carry.shape)\n",
        "    # print(x.shape)\n",
        "    # add layernorm\n",
        "    \n",
        "    x = nn.LayerNorm()(x)\n",
        "    x,_ = GRU(carry, x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.Dense(200)(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.LayerNorm()(x)\n",
        "    \n",
        "    x_L = nn.Dense(10)(x)\n",
        "    x_L = nn.relu(x_L)\n",
        "    x_L_loc = nn.Dense(1)(x_L)\n",
        "    # scale to 10-15\n",
        "    x_L_loc = nn.tanh(x_L_loc) * 2.5 + 12.5\n",
        "    x_L_scale = 0.5 * nn.Dense(1)(x_L)\n",
        "\n",
        "    x_A = nn.Dense(10)(x)\n",
        "    x_A = nn.relu(x_A)\n",
        "    x_A_loc = nn.Dense(1)(x_A)\n",
        "    x_A_scale = 0.5 * nn.Dense(1)(x_A)\n",
        "\n",
        "    x_T = nn.Dense(10)(x)\n",
        "    x_T = nn.relu(x_T)\n",
        "    x_T_loc = nn.Dense(1)(x_T)\n",
        "    # constrain to positive\n",
        "    x_T_loc = nn.softplus(x_T_loc)\n",
        "    x_T_scale = 0.5 * nn.Dense(1)(x_T)\n",
        "\n",
        "    x_kw = nn.Dense(10)(x)\n",
        "    x_kw = nn.relu(x_kw)\n",
        "    x_kw_loc = nn.Dense(1)(x_kw)\n",
        "    # scale to 0-2pi\n",
        "    x_kw_loc = nn.tanh(x_kw_loc) * jnp.pi + jnp.pi\n",
        "    x_kw_scale = 0.5 * nn.Dense(1)(x_kw)\n",
        "\n",
        "    x_ku = nn.Dense(10)(x)\n",
        "    x_ku = nn.relu(x_ku)\n",
        "    x_ku_loc = nn.Dense(1)(x_ku)\n",
        "    x_ku_scale = 0.5 * nn.Dense(1)(x_ku)\n",
        "\n",
        "    x_inc = nn.Dense(10)(x)\n",
        "    x_inc = nn.relu(x_inc)\n",
        "    x_inc_loc = nn.Dense(1)(x_inc)\n",
        "    # scale to 0-2pi\n",
        "    x_inc_loc = nn.tanh(x_inc_loc) * jnp.pi + jnp.pi\n",
        "    x_inc_scale = 0.5 * nn.Dense(1)(x_inc)\n",
        "\n",
        "    x_dr = nn.Dense(10)(x)\n",
        "    x_dr = nn.relu(x_dr)\n",
        "    x_dr_loc = nn.Dense(1)(x_dr)\n",
        "    # scale to 0.2-0.8\n",
        "    x_dr_loc = nn.tanh(x_dr_loc) * 0.3 + 0.5\n",
        "    x_dr_scale = 0.5 * nn.Dense(1)(x_dr)\n",
        "\n",
        "    x_phase_1 = nn.Dense(10)(x)\n",
        "    x_phase_1 = nn.relu(x_phase_1)\n",
        "    x_phase_1_loc = nn.Dense(1)(x_phase_1)\n",
        "    # scale to 0-2pi\n",
        "    x_phase_1_loc = nn.tanh(x_phase_1_loc) * jnp.pi + jnp.pi\n",
        "    x_phase_1_scale = 0.5 * nn.Dense(1)(x_phase_1)\n",
        "\n",
        "    x_phase_2 = nn.Dense(10)(x)\n",
        "    x_phase_2 = nn.relu(x_phase_2)\n",
        "    x_phase_2_loc = nn.Dense(1)(x_phase_2)\n",
        "    # scale to 0-2pi\n",
        "    x_phase_2_loc = nn.tanh(x_phase_2_loc) * jnp.pi + jnp.pi\n",
        "    x_phase_2_scale = 0.5 * nn.Dense(1)(x_phase_2)\n",
        "\n",
        "    x_phase_3 = nn.Dense(10)(x)\n",
        "    x_phase_3 = nn.relu(x_phase_3)\n",
        "    x_phase_3_loc = nn.Dense(1)(x_phase_3)\n",
        "    x_phase_3_scale = 0.5 * nn.Dense(1)(x_phase_3)\n",
        "\n",
        "    x_alpha = nn.Dense(10)(x)\n",
        "    x_alpha = nn.relu(x_alpha)\n",
        "    x_alpha_loc = nn.Dense(1)(x_alpha)\n",
        "    x_alpha_scale = 0.5 * nn.Dense(1)(x_alpha)\n",
        "\n",
        "    # return x_L_loc, jnp.exp(x_L_scale), x_A_loc, jnp.exp(x_A_scale), x_T_loc, jnp.exp(x_T_scale), x_kw_loc, jnp.exp(x_kw_scale), x_ku_loc, jnp.exp(x_ku_scale), x_inc_loc, jnp.exp(x_inc_scale), x_dr_loc, jnp.exp(x_dr_scale), x_phase_1_loc, jnp.exp(x_phase_1_scale), x_phase_2_loc, jnp.exp(x_phase_2_scale), x_phase_3_loc, jnp.exp(x_phase_3_scale), x_alpha_loc, jnp.exp(x_alpha_scale)\n",
        "    return x_L_loc.squeeze(-1), jnp.exp(x_L_scale.squeeze(-1)), x_A_loc.squeeze(-1), jnp.exp(x_A_scale.squeeze(-1)), x_T_loc.squeeze(-1), jnp.exp(x_T_scale.squeeze(-1)), x_kw_loc.squeeze(-1), jnp.exp(x_kw_scale.squeeze(-1)), x_ku_loc.squeeze(-1), jnp.exp(x_ku_scale.squeeze(-1)), x_inc_loc.squeeze(-1), jnp.exp(x_inc_scale.squeeze(-1)), x_dr_loc.squeeze(-1), jnp.exp(x_dr_scale.squeeze(-1)), x_phase_1_loc.squeeze(-1), jnp.exp(x_phase_1_scale.squeeze(-1)), x_phase_2_loc.squeeze(-1), jnp.exp(x_phase_2_scale.squeeze(-1)), x_phase_3_loc.squeeze(-1), jnp.exp(x_phase_3_scale.squeeze(-1)), x_alpha_loc.squeeze(-1), jnp.exp(x_alpha_scale.squeeze(-1))\n",
        "\n",
        "\n",
        "\n",
        "# class EncoderWhere(nn.Module):\n",
        "\n",
        "#   @nn.compact\n",
        "#   def __call__(self, frame_conv):\n",
        "#     x = frame_conv.reshape(frame_conv.shape[:-2] + (-1,))\n",
        "#     x = nn.softmax(x, -1)\n",
        "#     x = nn.Dense(200)(x)\n",
        "#     x = nn.relu(x)\n",
        "#     x = nn.Dense(200)(x)\n",
        "#     x = x.reshape(x.shape[:-1] + (2, 100))\n",
        "#     x = nn.relu(x)\n",
        "#     loc_raw = nn.Dense(2)(x[..., 0, :])\n",
        "#     scale_raw = 0.5 * nn.Dense(2)(x[..., 1, :])\n",
        "#     return nn.tanh(loc_raw), jnp.exp(scale_raw)\n",
        "\n",
        "class EncoderWhere(nn.Module):\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, frame_conv, worm_means = None):\n",
        "    x = frame_conv.reshape(frame_conv.shape[:-2] + (-1,))\n",
        "    # concatenate worm_means to x\n",
        "    if worm_means is None:\n",
        "      worm_means = jnp.zeros(x.shape[:-1] + (2,))\n",
        "    x = jnp.concatenate([x, jnp.zeros(x.shape[:-1] + (1,))], axis=-1)\n",
        "    x = nn.softmax(x, -1)\n",
        "    x = nn.Dense(200)(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.Dense(200)(x)\n",
        "    x = x.reshape(x.shape[:-1] + (2, 100))\n",
        "    x = nn.relu(x)\n",
        "    loc_raw = nn.Dense(2)(x[..., 0, :])\n",
        "    scale_raw = 0.5 * nn.Dense(2)(x[..., 1, :])\n",
        "    return nn.tanh(loc_raw), jnp.exp(scale_raw)\n",
        "\n",
        "\n",
        "class DecoderWhat(nn.Module):\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, z_what):\n",
        "    x = z_what.reshape(z_what.shape[:-2] + (-1,)) # flatten knots x 2 into vector\n",
        "    x = nn.Dense(200)(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.Dense(400)(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.Dense(784)(x)\n",
        "    logits = x.reshape(x.shape[:-1] + (28, 28))\n",
        "    return nn.sigmoid(logits)\n",
        "\n",
        "\n",
        "class BMNISTAutoEncoder(nn.Module):\n",
        "  digit_mean: jnp.ndarray\n",
        "  frame_size: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.encode_what = EncoderWhat()\n",
        "    self.encode_where = EncoderWhere()\n",
        "    self.decode_what = DecoderWhat()\n",
        "\n",
        "  def __call__(self, frames):\n",
        "    num_particles, batch_size, T, frame_size, frame_size = frames.shape\n",
        "    # Heuristic procedure to setup initial parameters.\n",
        "\n",
        "    z_where = []\n",
        "    for t in range(10):\n",
        "      frame = frames[..., t, :, :]\n",
        "      z_where_d = []\n",
        "      for d in range(2):\n",
        "        frames_conv = conv2d(frame, self.digit_mean)\n",
        "        z_where_t_d, _ = self.encode_where(frames_conv)\n",
        "        z_where_d.append(z_where_t_d)\n",
        "      z_where_d = jnp.stack(z_where_d, -2)\n",
        "      z_where.append(z_where_d)\n",
        "    # z_where = jnp.stack(z_where, -3).transpose(0, 2, 1, 3)\n",
        "    z_where = jnp.stack(z_where, -3).transpose(0, 1, 3, 2, 4)\n",
        "    print(z_where.shape)\n",
        "    digits = crop_frames(frames, z_where, 28)\n",
        "    print(digits.shape)\n",
        "    # z_what, _ = self.encode_what(digits)\n",
        "    proposed_sim_params = self.encode_what(digits)\n",
        "    print(proposed_sim_params[0].shape)\n",
        "    # proposed_sim_params = jax.tree_map(lambda x: x.squeeze(-1), proposed_sim_params)\n",
        "    # x0, y0 = z_where[..., 0, 0].squeeze(), z_where[..., 0, 1].squeeze()\n",
        "    x0, y0 = z_where[..., 0, 0], z_where[..., 0, 1]\n",
        "    assert x0.shape == proposed_sim_params[0].shape, f\"x0.shape: {x0.shape}, proposed_sim_params[0].shape: {proposed_sim_params[0].shape}\"\n",
        "    worm_sim = numpyro.handlers.condition(sim_worms, {'L': proposed_sim_params[0], 'A': proposed_sim_params[2], 'T': proposed_sim_params[4], 'kw': proposed_sim_params[6], 'ku': proposed_sim_params[8], 'inc': proposed_sim_params[10], 'dr': proposed_sim_params[12], 'phase_1': proposed_sim_params[14], 'phase_2': proposed_sim_params[16], 'phase_3': proposed_sim_params[18], 'alpha': proposed_sim_params[20], 'x0': x0, 'y0': y0})\n",
        "    worm_trace = numpyro.handlers.trace(worm_sim).get_trace(2, batch_size, T)\n",
        "    worms = worm_trace[\"worms\"][\"value\"]\n",
        "    print(worms.shape)\n",
        "    print(worms.mean(), worms.std())\n",
        "\n",
        "    digit_recon = self.decode_what(worms)\n",
        "    print(digit_recon.shape)\n",
        "    frames_recon = embed_digits(digit_recon, z_where, self.frame_size)\n",
        "    print(frames_recon.shape)\n",
        "    # check for nans\n",
        "    if jnp.any(jnp.isnan(frames_recon)):\n",
        "      print(\"frames_recon has nans\")\n",
        "    if jnp.any(jnp.isnan(digit_recon)):\n",
        "      print(\"digit_recon has nans\")\n",
        "    if jnp.any(jnp.isnan(worms)):\n",
        "      print(\"worms has nans\")\n",
        "      # print the proposed sim params where worms is nan\n",
        "      print(proposed_sim_params[0][jnp.isnan(worms)])\n",
        "    if jnp.any(jnp.isnan(digits)):\n",
        "      print(\"digits has nans\")\n",
        "    if jnp.any(jnp.isnan(frames)):\n",
        "      print(\"frames has nans\")\n",
        "    if jnp.any(jnp.isnan(z_where)):\n",
        "      print(\"z_where has nans\")\n",
        "    return frames_recon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we define the target and kernels as in Section 6.4.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def bmnist_target(network, inputs, batch_size=3, D=2, T=10):\n",
        "   \n",
        "  worms, x0s, y0s = sim_worms(D, batch_size, T)\n",
        "\n",
        "  z_where = []\n",
        "  # worm_frames = []\n",
        "  worm_frames = network.decode_what(worms)\n",
        "  for d in range(D):\n",
        "    z_where_d = []\n",
        "    # z_where_d_t = jnp.array([x0s[d], y0s[d]])\n",
        "    z_where_d_t = jnp.zeros(2)\n",
        "    for t in range(T):\n",
        "      # worm_frame = network.decode_what(worms[..., t, d, :, :])\n",
        "      # worm_frames.append(worm_frame)\n",
        "      scale = 1 if t == 0 else 0.1\n",
        "      z_where_d_t = numpyro.sample(\n",
        "          f\"z_where_{d}_{t}\", dist.Normal(z_where_d_t, scale).to_event(1)\n",
        "      )\n",
        "      z_where_d.append(z_where_d_t)\n",
        "    z_where_d = jnp.stack(z_where_d, -2)\n",
        "    z_where.append(z_where_d)\n",
        "  z_where = jnp.stack(z_where, -3)\n",
        "  z_where = z_where.squeeze() # this is a hack, there is some confusion with plating - the particle plate adds a singleton dimension\n",
        "\n",
        "  # print(\"worm_frames target\", worm_frames.shape)\n",
        "  # print(\"z_where target\", z_where.shape)\n",
        "  p = embed_digits(worm_frames, z_where, network.frame_size)\n",
        "  # print(\"p.shape target\", p.shape)\n",
        "  p = dist.util.clamp_probs(p.sum(-4))  # sum across digits\n",
        "  # print(\"summed p.shape target\", p.shape)\n",
        "  # print(\"inputs.shape\", inputs.shape)\n",
        "  frames = numpyro.sample(\"frames\", dist.Bernoulli(p).to_event(3), obs=inputs)\n",
        "\n",
        "  out = {\n",
        "      \"frames\": frames,\n",
        "      \"frames_recon\": p,\n",
        "      \"worms\": worms,\n",
        "      \"worm_frames\": jax.lax.stop_gradient(worm_frames),\n",
        "      **{f\"z_where_{t}\": z_where[..., t, :] for t in range(T)},\n",
        "  }\n",
        "  return (out,)\n",
        "\n",
        "\n",
        "def kernel_where(network, inputs, D=2, t=0, T=10):\n",
        "  if not isinstance(inputs, dict):\n",
        "    # print('making inputs')\n",
        "    inputs = {\n",
        "        \"frames\": inputs,\n",
        "        # \"worm_frames\": jnp.repeat(jnp.expand_dims(network.digit_mean, -3), D, -3),\n",
        "        \"worm_frames\": jnp.tile(network.digit_mean, (T, D, 1, 1)),\n",
        "        \"worms\": jnp.zeros((3, T, D, 6, 2)),\n",
        "    }\n",
        "  else:\n",
        "    pass\n",
        "    # print('inputs already made')\n",
        "\n",
        "  frame = inputs[\"frames\"][..., t, :, :]\n",
        "  z_where_t = []\n",
        "  worm_means = inputs[\"worms\"].mean(-2)\n",
        "  # print(\"worm_frames where\", inputs[\"worm_frames\"].shape)\n",
        "  # print(\"kernel worm means: \", worm_means.shape)\n",
        "  # print(\"input worm_frames shape\", inputs[\"worm_frames\"].shape)\n",
        "  \n",
        "  # with numpyro.plate('batch', 3, dim=-2):\n",
        "  #   with numpyro.plate('n_worms', D, dim=-1) as d:\n",
        "  #     digit = inputs[\"worm_frames\"][..., d, :, :]\n",
        "  #     x_conv = conv2d(frame, digit)\n",
        "  #     loc, scale = network.encode_where(x_conv, worm_means[..., t, d, :])\n",
        "  #     z_where_d_t = numpyro.sample(\n",
        "  #         f\"z_where_{d}_{t}\", dist.Normal(loc, scale).to_event(1)\n",
        "  #     )\n",
        "  #     z_where_t.append(z_where_d_t)\n",
        "  #     frame_recon = embed_digits(digit, z_where_d_t, network.frame_size)\n",
        "  #     frame = frame - frame_recon\n",
        "  for d in range(D):\n",
        "    worm_frame = inputs[\"worm_frames\"][..., t, d, :, :]\n",
        "    # print(\"worm_frame shape where\", worm_frame.shape)\n",
        "    # print(\"frame shape where\", frame.shape)\n",
        "    x_conv = conv2d(frame, worm_frame)\n",
        "    loc, scale = network.encode_where(x_conv, worm_means[..., t, d, :])\n",
        "    z_where_d_t = numpyro.sample(\n",
        "        f\"z_where_{d}_{t}\", dist.Normal(loc, scale).to_event(1)\n",
        "    )\n",
        "    z_where_t.append(z_where_d_t)\n",
        "    frame_recon = embed_digits(worm_frame, z_where_d_t, network.frame_size)\n",
        "    frame = frame - frame_recon\n",
        "  z_where_t = jnp.stack(z_where_t, -2)\n",
        "  z_where_t = z_where_t.squeeze() # this is a hack, there is some confusion with plating - the particle plate adds a singleton dimension\n",
        "  # print(\"z_where_t kernel where\", z_where_t.shape)\n",
        "  out = {**inputs, **{f\"z_where_{t}\": z_where_t}}\n",
        "  return (out,)\n",
        "\n",
        "\n",
        "# def kernel_what(network, inputs, T=10):\n",
        "#   z_where = jnp.stack([inputs[f\"z_where_{t}\"] for t in range(T)], -2)\n",
        "#   digits = crop_frames(inputs[\"frames\"], z_where, 28)\n",
        "#   loc, scale = network.encode_what(digits)\n",
        "#   z_what = numpyro.sample(\"z_what\", dist.Normal(loc, scale).to_event(2))\n",
        "\n",
        "#   out = {**inputs, **{\"z_what\": z_what}}\n",
        "#   return (out,)\n",
        "\n",
        "def kernel_what(network, inputs, T=10):\n",
        "  z_where = jnp.stack([inputs[f\"z_where_{t}\"] for t in range(T)], -2)\n",
        "  z_where = z_where.squeeze() # this is a hack, there is some confusion with plating - the particle plate adds a singleton dimension\n",
        "  # print(\"z_where kernel what\", z_where.shape)\n",
        "  worm_frames = crop_frames(inputs[\"frames\"], z_where, 28)\n",
        "  # print(\"worm_frames what\", worm_frames.shape)\n",
        "  proposed_sim_params = network.encode_what(worm_frames)\n",
        "  loc_L, scale_L, loc_A, scale_A, loc_T, scale_T, loc_kw, scale_kw, loc_ku, scale_ku, loc_inc, scale_inc, loc_dr, scale_dr, loc_phase_1, scale_phase_1, loc_phase_2, scale_phase_2, loc_phase_3, scale_phase_3, loc_alpha, scale_alpha = proposed_sim_params\n",
        "  # print(\"loc_L\", loc_L.shape)\n",
        "  with numpyro.plate('batch', inputs[\"frames\"].shape[0], dim=-2):\n",
        "    with numpyro.plate('n_worms', 2, dim=-1):\n",
        "\n",
        "      L = numpyro.sample('L', dist.TruncatedNormal(loc_L, scale_L, low=10, high=15))\n",
        "      A = numpyro.sample('A', dist.Normal(loc_A, scale_A))\n",
        "      T = numpyro.sample('T', dist.Normal(loc_T, scale_T))\n",
        "      kw = numpyro.sample('kw', dist.TruncatedNormal(loc_kw, scale_kw, low=0, high=2 * jnp.pi))\n",
        "      ku = numpyro.sample('ku', dist.Normal(loc_ku, scale_ku))\n",
        "      inc = numpyro.sample('inc', dist.TruncatedNormal(loc_inc, scale_inc, low=0, high=2 * jnp.pi))\n",
        "      dr = numpyro.sample('dr', dist.TruncatedNormal(loc_dr, scale_dr, low=0.2, high=0.8))\n",
        "      phase_1 = numpyro.sample('phase_1', dist.TruncatedNormal(loc_phase_1, scale_phase_1, low=0, high=2 * jnp.pi))\n",
        "      phase_2 = numpyro.sample('phase_2', dist.TruncatedNormal(loc_phase_2, scale_phase_2, low=0, high=2 * jnp.pi))\n",
        "      phase_3 = numpyro.sample('phase_3', dist.Normal(loc_phase_3, scale_phase_3))\n",
        "      alpha = numpyro.sample('alpha', dist.Normal(loc_alpha, scale_alpha))\n",
        "\n",
        "      # x0 = numpyro.sample('x0', dist.Delta(inputs[\"z_where_0\"][..., 0]))\n",
        "      # y0 = numpyro.sample('y0', dist.Delta(inputs[\"z_where_0\"][..., 1]))\n",
        "      x0 = numpyro.sample('x0', dist.Normal(inputs[\"z_where_0\"][..., 0], 0.1))\n",
        "      y0 = numpyro.sample('y0', dist.Normal(inputs[\"z_where_0\"][..., 1], 0.1))\n",
        "\n",
        "      # print(\"x0 shape\", x0.shape)\n",
        "\n",
        "  out = {**inputs, **{\"sim_vars\": {'L': L, 'A': A, 'T': T, 'kw': kw, 'ku': ku, 'inc': inc, 'dr': dr, 'phase_1': phase_1, 'phase_2': phase_2, 'phase_3': phase_3, 'alpha': alpha, 'x0': x0, 'y0': y0}}}\n",
        "  return (out,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we create the bmnist inference program, define the loss function,\n",
        "run the training loop, and plot the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_bmnist(params, bmnist_net, batch_size=3, T=10, num_sweeps=5, num_particles=10):\n",
        "  network = coix.util.BindModule(bmnist_net, params)\n",
        "  # Add particle dimension and construct a program.\n",
        "  make_particle_plate_0 = lambda: numpyro.plate(\"particle\", num_particles, dim=-2)\n",
        "  make_particle_plate = lambda: numpyro.plate(\"particle\", num_particles, dim=-3)\n",
        "  target = make_particle_plate()(partial(bmnist_target, network, batch_size=batch_size, D=2, T=T))\n",
        "  kernels = []\n",
        "  for t in range(T):\n",
        "    kernels.append(\n",
        "        # make_particle_plate()(partial(kernel_where, network, D=2, t=t))\n",
        "        make_particle_plate_0()(partial(kernel_where, network, D=2, t=t))\n",
        "    )\n",
        "  kernels.append(make_particle_plate()(partial(kernel_what, network, T=T)))\n",
        "  program = coix.algo.apgs(target, kernels, num_sweeps=num_sweeps)\n",
        "  return program\n",
        "\n",
        "\n",
        "def loss_fn(params, key, batch, bmnist_net, num_sweeps, num_particles):\n",
        "  # Prepare data for the program.\n",
        "  shuffle_rng, rng_key = random.split(key)\n",
        "  batch = random.permutation(shuffle_rng, batch, axis=1)\n",
        "  T = batch.shape[-3]\n",
        "  batch_size = batch.shape[-4]\n",
        "\n",
        "  # Run the program and get metrics.\n",
        "  program = make_bmnist(params, bmnist_net, batch_size, T, num_sweeps, num_particles)\n",
        "  _, _, metrics = coix.traced_evaluate(program, seed=rng_key)(batch)\n",
        "  for metric_name in [\"log_Z\", \"log_density\", \"loss\"]:\n",
        "    metrics[metric_name] = metrics[metric_name] / batch.shape[0]\n",
        "  return metrics[\"loss\"], metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Args(argparse.Namespace):\n",
        "  batch_size = 5\n",
        "  # batch_size = 3\n",
        "  num_sweeps = 5\n",
        "  num_particles = 10\n",
        "  learning_rate = 1e-4\n",
        "  num_steps = 20000\n",
        "  # num_steps = 200\n",
        "  device = \"gpu\"\n",
        "\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1722631731.969817  250235 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-08-02 22:48:51.996476: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2024-08-02 22:48:53.105519: W external/xla/xla/service/gpu/nvptx_compiler.cc:836] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version (12.5.82). Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10, 5, 2, 10, 2)\n",
            "(10, 5, 2, 10, 28, 28)\n",
            "(10, 5, 2)\n",
            "(10, 5, 2, 10, 6, 2)\n",
            "(10, 5, 2, 10, 28, 28)\n",
            "(10, 5, 2, 10, 64, 64)\n"
          ]
        }
      ],
      "source": [
        "lr = args.learning_rate\n",
        "num_steps = args.num_steps\n",
        "batch_size = args.batch_size\n",
        "num_sweeps = args.num_sweeps\n",
        "num_particles = args.num_particles\n",
        "\n",
        "train_ds = load_dataset(is_training=True, batch_size=batch_size)\n",
        "\n",
        "test_ds = load_dataset(is_training=False, batch_size=batch_size)\n",
        "digit_mean = get_digit_mean()\n",
        "test_data = next(test_ds)\n",
        "frame_size = test_data.shape[-1]\n",
        "bmnist_net = BMNISTAutoEncoder(digit_mean=digit_mean, frame_size=frame_size)\n",
        "test_data_tiled = np.tile(test_data, ((num_particles,) + ((1,) * test_data.ndim)))\n",
        "init_params = bmnist_net.init(jax.random.PRNGKey(0), test_data_tiled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr_schedule = cosine_decay_schedule(lr, num_steps, 0.01)\n",
        "\n",
        "opt = optax.chain(\n",
        "    clip_by_global_norm(1.0),\n",
        "    optax.adam(lr_schedule),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compiling the first train step...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-08-02 22:51:41.083096: W external/xla/xla/service/hlo_rematerialization.cc:3005] Can't reduce memory use below 16.84GiB (18084178587 bytes) by rematerialization; only reduced to 17.58GiB (18877517887 bytes), down from 17.61GiB (18912992259 bytes) originally\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time to compile a train step: 212.9702091217041\n",
            "=====\n"
          ]
        }
      ],
      "source": [
        "\n",
        "bmnist_params, _ = coix.util.train(\n",
        "    partial(\n",
        "        loss_fn,\n",
        "        bmnist_net=bmnist_net,\n",
        "        num_sweeps=num_sweeps,\n",
        "        num_particles=num_particles,\n",
        "    ),\n",
        "    init_params,\n",
        "    # optax.adam(lr),\n",
        "    opt,\n",
        "    num_steps,\n",
        "    train_ds,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5, 10, 64, 64)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load worm_learned_params.npy\n",
        "\n",
        "# bmnist_params = np.load(\"worm_learned_params.npy\", allow_pickle=True).item()\n",
        "\n",
        "np.save(\"worm_learned_params.npy\", bmnist_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T_test = test_data.shape[-3]\n",
        "batch_size_test = test_data.shape[-4]\n",
        "program = make_bmnist(\n",
        "    bmnist_params, bmnist_net, batch_size_test, T_test, num_sweeps, num_particles\n",
        ")\n",
        "out, _, _ = coix.traced_evaluate(program, seed=jax.random.PRNGKey(1))(\n",
        "    test_data\n",
        ")\n",
        "out = out[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "b = 1\n",
        "prop_cycle = plt.rcParams[\"axes.prop_cycle\"]\n",
        "colors = prop_cycle.by_key()[\"color\"]\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "def animate(i):\n",
        "  axes[0].cla()\n",
        "  axes[0].imshow(test_data[b, i])\n",
        "  axes[1].cla()\n",
        "  axes[1].imshow(out[\"frames_recon\"][0, b, i])\n",
        "  for d in range(2):\n",
        "    where = 0.5 * (out[f\"z_where_{i}\"][0, b, d] + 1) * (frame_size - 28) - 0.5\n",
        "    color = colors[d]\n",
        "    axes[0].add_patch(\n",
        "        Rectangle(where, 28, 28, edgecolor=color, lw=3, fill=False)\n",
        "    )\n",
        "plt.rc(\"animation\", html=\"jshtml\")\n",
        "plt.tight_layout()\n",
        "ani = animation.FuncAnimation(fig, animate, frames=range(10), interval=300)\n",
        "writer = animation.PillowWriter(fps=15)\n",
        "ani.save(\"bmnist.gif\", writer=writer)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# network = coix.util.BindModule(bmnist_net, bmnist_params)\n",
        "\n",
        "# out_z_where = jnp.stack([out[f\"z_where_{i}\"] for i in range(10)], -2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
