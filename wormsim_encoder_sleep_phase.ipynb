{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Simulator-based unsupervised detection and tracking of worms\n",
        "\n",
        "**References**\n",
        "\n",
        "    1. Wu, Hao, et al. Amortized population Gibbs samplers with neural\n",
        "       sufficient statistics. ICML 2020.\n",
        "\n",
        "<img src=\"file://../_static/wormsim.gif\" align=\"center\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/frans/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2024-08-20 09:06:07.655495: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-20 09:06:07.688923: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-20 09:06:07.699570: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-20 09:06:08.624166: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "from functools import partial, reduce\n",
        "\n",
        "import coix\n",
        "from coix.core import detach\n",
        "from coix.api import compose, propose\n",
        "from coix import util\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "from jax import random\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib.patches import Rectangle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpyro\n",
        "import numpyro.distributions as dist\n",
        "import optax\n",
        "from optax import cosine_decay_schedule\n",
        "from optax import clip_by_global_norm\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from sim_utils import *\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def dataset_generator(file_path, n_data=-1):\n",
        "    ds = np.load(file_path, mmap_mode='r')\n",
        "    ds = ds[:n_data] if n_data != -1 else ds\n",
        "    for data in ds:\n",
        "        yield data\n",
        "\n",
        "def load_dataset(*, is_training, batch_size, n_data=-1, file_path=\"worms_train_40k.npy\"):\n",
        "    # Create a dataset from the generator\n",
        "    ds = tf.data.Dataset.from_generator(\n",
        "        dataset_generator,\n",
        "        args=(file_path, n_data),\n",
        "        output_signature=tf.TensorSpec(shape=(None, None, None), dtype=tf.float32)\n",
        "    )\n",
        "    \n",
        "    ds = ds.repeat()\n",
        "    if is_training:\n",
        "        ds = ds.shuffle(10 * batch_size, seed=0)\n",
        "    ds = ds.batch(batch_size)\n",
        "    \n",
        "    # Standardize the data between 0 and 1\n",
        "    ds = ds.map(lambda x: x / 0.80999994)\n",
        "    return iter(tfds.as_numpy(ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vmapped_sim_fn(sim_fn, params):\n",
        "    if params['L'].ndim == 1:\n",
        "        return jax.vmap(sim_fn, in_axes=0, out_axes=0)(params)\n",
        "    else:\n",
        "        return jax.vmap(partial(vmapped_sim_fn, sim_fn), in_axes=0, out_axes=0)(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sim_worms(nworms, n_frames):\n",
        "    duration = 1.55\n",
        "    snapshots = 10\n",
        "    kpoints = 6\n",
        "    L_low = 23\n",
        "    L_high = 28\n",
        "    L = numpyro.sample('L', dist.Uniform(L_low, L_high).expand([nworms]).to_event())\n",
        "    A = numpyro.sample('A', dist.Normal(1, 0.1).expand([nworms]).to_event())\n",
        "    T = numpyro.sample('T', dist.Normal(0.8, 0.1).expand([nworms]).to_event())\n",
        "    kw = numpyro.sample('kw', dist.Uniform(0, 2 * jnp.pi).expand([nworms]).to_event())\n",
        "    ku = numpyro.sample('ku', dist.Normal(jnp.pi, 1).expand([nworms]).to_event())\n",
        "\n",
        "    inc = numpyro.sample('inc', dist.Uniform(0, 2 * jnp.pi).expand([nworms]).to_event())\n",
        "    dr = numpyro.sample('dr', dist.Uniform(0.2, 0.8).expand([nworms]).to_event())\n",
        "    phase_1 = numpyro.sample('phase_1', dist.Uniform(0, 2 * jnp.pi).expand([nworms]).to_event())\n",
        "    phase_2 = numpyro.sample('phase_2', dist.Uniform(0, 2 * jnp.pi).expand([nworms]).to_event())\n",
        "    phase_3 = numpyro.sample('phase_3', dist.Normal(0, 0.1).expand([nworms]).to_event())\n",
        "    alpha = numpyro.sample('alpha', dist.Normal(4, 4).expand([nworms]).to_event())\n",
        "    alpha = jnp.abs(alpha + 1.0)\n",
        "\n",
        "    x0 = jnp.zeros_like(L)\n",
        "    y0 = jnp.zeros_like(L)\n",
        "\n",
        "    params = {'L': L, 'A': A, 'T': T, 'kw': kw, 'ku': ku, 'inc': inc, 'dr': dr, 'phase_1': phase_1, 'phase_2': phase_2, 'phase_3': phase_3, 'alpha': alpha, 'x0': x0, 'y0': y0}\n",
        "    sim_fn = partial(\n",
        "        worm_simulation,\n",
        "        duration=duration,\n",
        "        snapshots=snapshots,\n",
        "        kpoints=kpoints,\n",
        "    )\n",
        "    worms = vmapped_sim_fn(sim_fn, params)\n",
        "    worms = worms / ((L_high + L_low) / 2)\n",
        "    numpyro.deterministic('worms', worms)\n",
        "    return worms, params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def scale_and_translate(image, where, out_size):\n",
        "  translate = abs(image.shape[-1] - out_size) * (where[..., ::-1] + 1) / 2\n",
        "  return jax.image.scale_and_translate(\n",
        "      image,\n",
        "      (out_size, out_size),\n",
        "      (0, 1),\n",
        "      jnp.ones(2),\n",
        "      translate,\n",
        "      method=\"cubic\",\n",
        "      antialias=False,\n",
        "  )\n",
        "\n",
        "def scale_and_translate_variable_scale(image, where, scale, out_size):\n",
        "  translate = abs(image.shape[-1] - out_size) * (where[..., ::-1] + 1) / 2\n",
        "  return jax.image.scale_and_translate(\n",
        "      image,\n",
        "      (out_size, out_size),\n",
        "      (0, 1),\n",
        "      scale * jnp.ones(2),\n",
        "      translate,\n",
        "      method=\"cubic\",\n",
        "      antialias=False,\n",
        "  )\n",
        "\n",
        "def crop_frames(frames, z_where, digit_size=28):\n",
        "  # frames:           time.frame_size.frame_size\n",
        "  # z_where: (worm_frames).time.2\n",
        "  # out:     (digits).time.digit_size.digit_size\n",
        "  if frames.ndim == 2 and z_where.ndim == 1:\n",
        "    return scale_and_translate(frames, z_where, out_size=digit_size)\n",
        "  elif frames.ndim == 3 and z_where.ndim == 2:\n",
        "    in_axes = (0, 0)\n",
        "  elif frames.ndim == 4 and z_where.ndim == 3:\n",
        "    in_axes = (0, 0)\n",
        "  elif frames.ndim == 2 and z_where.ndim == 2:\n",
        "    in_axes = (None, 0)\n",
        "  elif frames.ndim == z_where.ndim:\n",
        "    in_axes = (0, 0)\n",
        "  elif frames.ndim > z_where.ndim:\n",
        "    in_axes = (0, None)\n",
        "  else:\n",
        "    in_axes = (None, 0)\n",
        "  return jax.vmap(partial(crop_frames, digit_size=digit_size), in_axes)(\n",
        "      frames, z_where\n",
        "  )\n",
        "\n",
        "\n",
        "def embed_frames(worm_frames, z_where, frame_size=64):\n",
        "  # worm_frames:  (worm_frames).      .digit_size.digit_size\n",
        "  # z_where: (worm_frames).(time).2\n",
        "  # out:     (worm_frames).(time).frame_size.frame_size\n",
        "  if worm_frames.ndim == 2 and z_where.ndim == 1:\n",
        "    return scale_and_translate(worm_frames, z_where, out_size=frame_size)\n",
        "  elif worm_frames.ndim == 2 and z_where.ndim == 2:\n",
        "    in_axes = (None, 0)\n",
        "  elif worm_frames.ndim >= z_where.ndim:\n",
        "    in_axes = (0, 0)\n",
        "  else:\n",
        "    in_axes = (None, 0)\n",
        "  return jax.vmap(partial(embed_frames, frame_size=frame_size), in_axes)(\n",
        "      worm_frames, z_where\n",
        "  )\n",
        "\n",
        "def embed_worms(worm_frames, z_where, scale, frame_size=64):\n",
        "  # worm_frames:  (worm_frames).      .digit_size.digit_size\n",
        "  # z_where: (worm_frames).(time).2\n",
        "  # out:     (worm_frames).(time).frame_size.frame_size\n",
        "  if worm_frames.ndim == 2 and z_where.ndim == 1:\n",
        "    return scale_and_translate_variable_scale(worm_frames, z_where, scale, out_size=frame_size)\n",
        "  elif worm_frames.ndim == 2 and z_where.ndim == 2:\n",
        "    in_axes = (None, 0, 0)\n",
        "  elif worm_frames.ndim >= z_where.ndim:\n",
        "    in_axes = (0, 0, 0)\n",
        "  else:\n",
        "    in_axes = (None, 0, 0)\n",
        "  return jax.vmap(partial(embed_worms, frame_size=frame_size), in_axes)(\n",
        "      worm_frames, z_where, scale\n",
        "  )\n",
        "\n",
        "def conv2d(frames, worm_frames):\n",
        "  # frames:          (time).frame_size.frame_size\n",
        "  # worm_frames: (worm_frames).      .digit_size.digit_size\n",
        "  # out:    (worm_frames).(time).conv_size .conv_size\n",
        "  if frames.ndim == 2 and worm_frames.ndim == 2:\n",
        "    return jax.scipy.signal.convolve2d(frames, worm_frames, mode=\"valid\")\n",
        "  elif frames.ndim == worm_frames.ndim:\n",
        "    in_axes = (0, 0)\n",
        "  elif frames.ndim > worm_frames.ndim:\n",
        "    in_axes = (0, None)\n",
        "  else:\n",
        "    in_axes = (None, 0)\n",
        "  return jax.vmap(conv2d, in_axes=in_axes)(frames, worm_frames)\n",
        "\n",
        "def resize_batch(frames, size):\n",
        "  if frames.ndim == 2:\n",
        "    return jax.image.resize(frames, (size, size), method=\"cubic\")\n",
        "  elif frames.ndim > 2:\n",
        "    return jax.vmap(partial(resize_batch, size=size))(frames)\n",
        "\n",
        "# interpolate the sparse points outputted by the simulator\n",
        "def interpolate(worms, n_points=32):\n",
        "  if worms.ndim == 1:\n",
        "    return jnp.interp(jnp.linspace(0, 1, n_points), jnp.linspace(0, 1, worms.shape[0]), worms)\n",
        "  elif worms.ndim == 2:\n",
        "    return jax.vmap(partial(interpolate, n_points=n_points), in_axes=1, out_axes=1)(worms)\n",
        "  else:\n",
        "    return jax.vmap(partial(interpolate, n_points=n_points), in_axes=0)(worms)\n",
        "  \n",
        "# def coords2vec(coords, frame_size=28):\n",
        "#     foo = jnp.zeros((frame_size, frame_size))\n",
        "#     idxs = (coords.reshape(6, 2) * frame_size + frame_size/2).astype(int)\n",
        "#     for i, idx in enumerate(idxs):\n",
        "#         foo = foo.at[idx[1], idx[0]].set(1)    \n",
        "#     return foo.reshape(frame_size*frame_size)\n",
        "\n",
        "# def vmapped_coords2vec(coords, frame_size=28):\n",
        "#     if coords.ndim == 1:\n",
        "#         return coords2vec(coords, frame_size)\n",
        "#     else:\n",
        "#         return jax.vmap(partial(vmapped_coords2vec, frame_size=frame_size), in_axes=0, out_axes=0)(coords)\n",
        "    \n",
        "# def vec2coords(vec, frame_size=28):\n",
        "#   img = vec.reshape(frame_size, frame_size)\n",
        "#   return (jnp.argwhere(img, size=6)[::-1, [1, 0]] - (frame_size/2)) / frame_size\n",
        "\n",
        "# def vmapped_vec2coords(vec, frame_size=28):\n",
        "#     if vec.ndim == 1:\n",
        "#         return vec2coords(vec, frame_size)\n",
        "#     else:\n",
        "#         return jax.vmap(partial(vmapped_vec2coords, frame_size=frame_size), in_axes=0, out_axes=0)(vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderSim(nn.Module):\n",
        "  \"\"\"\n",
        "  Takes sequence of z_what and encodes them into a distribution over worm simulator parameters\n",
        "  \"\"\"\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, z_what, carry=None):\n",
        "    broadcast_dims = z_what.shape[:-2]\n",
        "    \n",
        "    hidden_dim = 512\n",
        "    \n",
        "    # x = z_what.reshape(z_what.shape[:-2] + (-1,)) # flatten frame into vector\n",
        "    x = z_what\n",
        "    \n",
        "    if carry is None:\n",
        "      carry = self.param('carry_init', \n",
        "                         lambda rng, shape: jnp.zeros(shape), \n",
        "                         (hidden_dim,))\n",
        "    GRU = nn.scan(nn.GRUCell,\n",
        "                  in_axes=-2,\n",
        "                  out_axes=-2,\n",
        "                  variable_broadcast='params',\n",
        "                  split_rngs={'params': False}\n",
        "                  )(hidden_dim)\n",
        "    \n",
        "    x = nn.LayerNorm()(x)\n",
        "    # tile the carry state to match the batch size\n",
        "    carry = jnp.tile(carry, broadcast_dims + (1,))\n",
        "    carry, x = GRU(carry, x)\n",
        "    # x = x[..., -1, :] # use the last time step\n",
        "    x = x.sum(-2) # sum across time\n",
        "    x = nn.Dense(64)(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.LayerNorm()(x)\n",
        "    \n",
        "    x_L = nn.Dense(10)(x)\n",
        "    x_L = nn.relu(x_L)\n",
        "    x_L_loc = nn.Dense(1)(x_L)\n",
        "    # scale to 10-15\n",
        "    x_L_loc = nn.tanh(x_L_loc) * ((28 - 23) / 2) + (23 + 28) / 2\n",
        "    x_L_scale = 0.5 * nn.Dense(1)(x_L)\n",
        "\n",
        "    x_A = nn.Dense(10)(x)\n",
        "    x_A = nn.relu(x_A)\n",
        "    x_A_loc = nn.Dense(1)(x_A)\n",
        "    x_A_scale = 0.5 * nn.Dense(1)(x_A)\n",
        "\n",
        "    x_T = nn.Dense(10)(x)\n",
        "    x_T = nn.relu(x_T)\n",
        "    x_T_loc = nn.Dense(1)(x_T)\n",
        "    # constrain to positive\n",
        "    x_T_loc = nn.softplus(x_T_loc)\n",
        "    x_T_scale = 0.5 * nn.Dense(1)(x_T)\n",
        "\n",
        "    x_kw = nn.Dense(10)(x)\n",
        "    x_kw = nn.relu(x_kw)\n",
        "    x_kw_loc = nn.Dense(1)(x_kw)\n",
        "    # scale to 0-2pi\n",
        "    x_kw_loc = nn.tanh(x_kw_loc) * jnp.pi + jnp.pi\n",
        "    x_kw_scale = 0.5 * nn.Dense(1)(x_kw)\n",
        "\n",
        "    x_ku = nn.Dense(10)(x)\n",
        "    x_ku = nn.relu(x_ku)\n",
        "    x_ku_loc = nn.Dense(1)(x_ku)\n",
        "    x_ku_scale = 0.5 * nn.Dense(1)(x_ku)\n",
        "\n",
        "    x_inc = nn.Dense(10)(x)\n",
        "    x_inc = nn.relu(x_inc)\n",
        "    x_inc_loc = nn.Dense(1)(x_inc)\n",
        "    # scale to 0-2pi\n",
        "    x_inc_loc = nn.tanh(x_inc_loc) * jnp.pi + jnp.pi\n",
        "    x_inc_scale = 0.5 * nn.Dense(1)(x_inc)\n",
        "\n",
        "    x_dr = nn.Dense(10)(x)\n",
        "    x_dr = nn.relu(x_dr)\n",
        "    x_dr_loc = nn.Dense(1)(x_dr)\n",
        "    # scale to 0.2-0.8\n",
        "    x_dr_loc = nn.tanh(x_dr_loc) * 0.3 + 0.5\n",
        "    x_dr_scale = 0.5 * nn.Dense(1)(x_dr)\n",
        "\n",
        "    x_phase_1 = nn.Dense(10)(x)\n",
        "    x_phase_1 = nn.relu(x_phase_1)\n",
        "    x_phase_1_loc = nn.Dense(1)(x_phase_1)\n",
        "    # scale to 0-2pi\n",
        "    x_phase_1_loc = nn.tanh(x_phase_1_loc) * jnp.pi + jnp.pi\n",
        "    x_phase_1_scale = 0.5 * nn.Dense(1)(x_phase_1)\n",
        "\n",
        "    x_phase_2 = nn.Dense(10)(x)\n",
        "    x_phase_2 = nn.relu(x_phase_2)\n",
        "    x_phase_2_loc = nn.Dense(1)(x_phase_2)\n",
        "    # scale to 0-2pi\n",
        "    x_phase_2_loc = nn.tanh(x_phase_2_loc) * jnp.pi + jnp.pi\n",
        "    x_phase_2_scale = 0.5 * nn.Dense(1)(x_phase_2)\n",
        "\n",
        "    x_phase_3 = nn.Dense(10)(x)\n",
        "    x_phase_3 = nn.relu(x_phase_3)\n",
        "    x_phase_3_loc = nn.Dense(1)(x_phase_3)\n",
        "    x_phase_3_scale = 0.5 * nn.Dense(1)(x_phase_3)\n",
        "\n",
        "    x_alpha = nn.Dense(10)(x)\n",
        "    x_alpha = nn.relu(x_alpha)\n",
        "    x_alpha_loc = nn.Dense(1)(x_alpha)\n",
        "    x_alpha_scale = 0.5 * nn.Dense(1)(x_alpha)\n",
        "\n",
        "    return x_L_loc.squeeze(-1), jnp.exp(x_L_scale.squeeze(-1)), x_A_loc.squeeze(-1), jnp.exp(x_A_scale.squeeze(-1)), x_T_loc.squeeze(-1), jnp.exp(x_T_scale.squeeze(-1)), x_kw_loc.squeeze(-1), jnp.exp(x_kw_scale.squeeze(-1)), x_ku_loc.squeeze(-1), jnp.exp(x_ku_scale.squeeze(-1)), x_inc_loc.squeeze(-1), jnp.exp(x_inc_scale.squeeze(-1)), x_dr_loc.squeeze(-1), jnp.exp(x_dr_scale.squeeze(-1)), x_phase_1_loc.squeeze(-1), jnp.exp(x_phase_1_scale.squeeze(-1)), x_phase_2_loc.squeeze(-1), jnp.exp(x_phase_2_scale.squeeze(-1)), x_phase_3_loc.squeeze(-1), jnp.exp(x_phase_3_scale.squeeze(-1)), x_alpha_loc.squeeze(-1), jnp.exp(x_alpha_scale.squeeze(-1))\n",
        "\n",
        "class EncoderWhat(nn.Module):\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, digits):\n",
        "    x = jnp.expand_dims(digits, -1)\n",
        "    x = nn.Conv(features=32, kernel_size=(3, 3), strides=(1, 1))(x)\n",
        "    # x = nn.Conv(features=4, kernel_size=(3, 3), strides=(1, 1))(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.Conv(features=64, kernel_size=(3, 3), strides=(2, 2))(x)\n",
        "    # x = nn.Conv(features=8, kernel_size=(3, 3), strides=(2, 2))(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.Conv(features=128, kernel_size=(3, 3), strides=(2, 2))(x)\n",
        "    # x = nn.Conv(features=16, kernel_size=(3, 3), strides=(2, 2))(x)\n",
        "    x = nn.relu(x)\n",
        "    # flatten\n",
        "    x = x.reshape(x.shape[:-3] + (-1,))\n",
        "    x = nn.Dense(512)(x)\n",
        "    x = nn.relu(x)\n",
        "    loc_raw = nn.Dense(14*14)(x)\n",
        "    loc_raw = nn.sigmoid(loc_raw)\n",
        "    scale_raw = 0.5 * nn.Dense(14*14)(x)\n",
        "    return loc_raw, jnp.exp(scale_raw)\n",
        "\n",
        "class EncoderWhere(nn.Module):\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, frame_conv):\n",
        "    x = jnp.expand_dims(frame_conv, -1)\n",
        "    x = nn.Conv(features=64, kernel_size=(3, 3), strides=(2, 2))(x)\n",
        "    x = nn.relu(x) \n",
        "    x = nn.Conv(features=32, kernel_size=(3, 3), strides=(2, 2))(x)\n",
        "    x = nn.relu(x) \n",
        "    x = x.reshape(x.shape[:-3] + (-1,))\n",
        "    x = nn.Dense(64)(x)\n",
        "    x = x.reshape(x.shape[:-1] + (2, 32))\n",
        "    x = nn.relu(x)\n",
        "    loc_raw = nn.Dense(2)(x[..., 0, :])\n",
        "    scale_raw = 0.5 * nn.Dense(2)(x[..., 1, :])\n",
        "    return nn.tanh(loc_raw), jnp.exp(scale_raw)\n",
        "\n",
        "class DecoderWhat(nn.Module):\n",
        "  \"\"\"\n",
        "  Hardcoded decoder to plot worm coordinates on a frame\n",
        "  \"\"\"\n",
        "  @nn.compact\n",
        "  def __call__(self, worms):\n",
        "    # vmap interpolate over all worms\n",
        "    worms = interpolate(worms, n_points = 16)    \n",
        "\n",
        "    # make worm widths for all knots\n",
        "    # R = 0.8\n",
        "    R = nn.softplus(nn.Dense(1)(jnp.ones(1))) + 0.7 # we need some params for the module to be registered\n",
        "    # worm_scale = nn.softplus(nn.Dense(1)(jnp.ones(1))) * 2\n",
        "    worm_scale = 2.5\n",
        "    # print(\"R:\", R)\n",
        "    K = worms.shape[-2]\n",
        "    i = jnp.arange(K)\n",
        "    r = R * jnp.abs(jnp.sin(jnp.arccos((i - K / 2) / (K / 2 + 0.2))))\n",
        "    r = jnp.tile(r, (worms.shape[:-2] + (1,)))\n",
        "\n",
        "    # draw the circles\n",
        "    # circles = embed_worms(circle_image(4), worms * worm_scale, r, frame_size=28)\n",
        "    circles = embed_worms(jnp.ones((2, 2)), worms * worm_scale, r, frame_size=28)\n",
        "    # overlay the circles\n",
        "    p = circles.sum(-3)\n",
        "    p = p ** 0.001\n",
        "    p -= p.min()\n",
        "    p = p / p.max()\n",
        "    return p\n",
        "\n",
        "# class DecoderWhat(nn.Module):\n",
        "#   \"\"\"\n",
        "#   Hardcoded decoder to plot worm coordinates on a frame\n",
        "#   \"\"\"\n",
        "#   @nn.compact\n",
        "#   def __call__(self, worms):\n",
        "#     # vmap interpolate over all worms\n",
        "#     worms = interpolate(worms, n_points = 12)    \n",
        "\n",
        "#     # make worm widths for all knots\n",
        "#     # R = 0.8\n",
        "#     R = nn.softplus(nn.Dense(1)(jnp.ones(1))) + 0.6\n",
        "#     worm_scale = nn.softplus(nn.Dense(1)(jnp.ones(1))) * 2\n",
        "#     # print(\"R:\", R)\n",
        "#     K = worms.shape[-2]\n",
        "#     i = jnp.arange(K)\n",
        "#     r = R * jnp.abs(jnp.sin(jnp.arccos((i - K / 2) / (K / 2 + 0.2))))\n",
        "#     r = jnp.tile(r, (worms.shape[:-2] + (1,)))\n",
        "\n",
        "#     # draw the circles\n",
        "#     circles = embed_worms(circle_image(4), worms * worm_scale, r, frame_size=28)\n",
        "#     # print(\"circles.shape\", circles.shape)\n",
        "#     circles_sum_flatten = circles.sum(-3).reshape(circles.shape[:-3] + (-1,))\n",
        "#     # overlay the circles\n",
        "#     p = nn.sigmoid(nn.Dense(28 * 28)(circles_sum_flatten))\n",
        "#     p = p.reshape(circles_sum_flatten.shape[:-1] + (28, 28))\n",
        "#     return p\n",
        "\n",
        "# class DecoderWhat(nn.Module):\n",
        "\n",
        "#   @nn.compact\n",
        "#   def __call__(self, z_what):\n",
        "#     # x = nn.Dense(3136)(z_what)\n",
        "#     x = nn.Dense(784)(z_what)\n",
        "#     x = nn.relu(x)\n",
        "#     # x = x.reshape(x.shape[:-1] + (7, 7, 64))\n",
        "#     x = x.reshape(x.shape[:-1] + (7, 7, 16))\n",
        "#     x = nn.ConvTranspose(features=32, kernel_size=(3, 3), strides=(2, 2))(x)\n",
        "#     x = nn.relu(x)\n",
        "#     x = nn.ConvTranspose(features=1, kernel_size=(3, 3), strides=(2, 2))(x)\n",
        "#     x = nn.relu(x)\n",
        "#     logits = x.squeeze(-1)\n",
        "#     p = nn.sigmoid(logits)\n",
        "#     return p\n",
        "\n",
        "class wormsimAutoEncoder(nn.Module):\n",
        "  num_particles: int\n",
        "  batch_size: int\n",
        "  frame_size: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.encode_sim = EncoderSim()\n",
        "    self.encode_what = EncoderWhat()\n",
        "    self.encode_where = EncoderWhere()\n",
        "    self.decode_what = DecoderWhat()\n",
        "\n",
        "  def __call__(self, frames):\n",
        "    \"\"\"\n",
        "    Only used to initialize the model\n",
        "    \"\"\"\n",
        "    T = 10\n",
        "    batch_size = frames.shape[0]\n",
        "    \n",
        "    # print(\"frames.shape\", frames.shape)\n",
        "    resized_frames = resize_batch(frames, 32)\n",
        "    # print(\"resized_frames.shape\", resized_frames.shape)\n",
        "    z_where, _ = self.encode_where(resized_frames)\n",
        "    worm_frames = crop_frames(frames, -2-z_where)\n",
        "    # print(\"worm_frames.shape\", worm_frames.shape)\n",
        "    z_what, _ = self.encode_what(worm_frames)\n",
        "\n",
        "    # print(\"z_what_tile.shape\", z_what_tile.shape)\n",
        "    proposed_sim_params = self.encode_sim(z_what)\n",
        "    worm_sim = numpyro.handlers.condition(sim_worms, {'L': proposed_sim_params[0], 'A': proposed_sim_params[2], 'T': proposed_sim_params[4], 'kw': proposed_sim_params[6], 'ku': proposed_sim_params[8], 'inc': proposed_sim_params[10], 'dr': proposed_sim_params[12], 'phase_1': proposed_sim_params[14], 'phase_2': proposed_sim_params[16], 'phase_3': proposed_sim_params[18], 'alpha': proposed_sim_params[20]})\n",
        "    worm_trace = numpyro.handlers.trace(worm_sim).get_trace(2, T)\n",
        "    worms = worm_trace[\"worms\"][\"value\"]\n",
        "\n",
        "    # print(\"L shape\", proposed_sim_params[0].shape)\n",
        "\n",
        "    # # z_what is normally sampled from a Gaussian with mean worms, \n",
        "    # # but doesn't matter for the purpose of initializing NNs\n",
        "\n",
        "    # print(\"reshape_z_what.shape\", reshape_z_what.shape)\n",
        "    worm_frame_recon = self.decode_what(worms)\n",
        "    # worm_frame_recon = self.decode_what(z_what)\n",
        "    frames_recon = embed_frames(worm_frame_recon, z_where, self.frame_size)\n",
        "    # print(\"frames_recon.shape\", frames_recon.shape)\n",
        "    return frames_recon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we define the target and kernels as in Section 6.4.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def wormsim_target(network, inputs, D=2, T=10, sleep_phase = False):\n",
        "\n",
        "  worms, params = sim_worms(D, T)\n",
        "\n",
        "  # print(\"L target\", params['L'][0, 0])\n",
        "\n",
        "  z_where = []\n",
        "  for d in range(D):\n",
        "    z_where_d = []\n",
        "    z_where_d_t = jnp.zeros(2)\n",
        "    for t in range(T):\n",
        "      scale = 1 if t == 0 else 0.05\n",
        "      z_where_d_t = numpyro.sample(\n",
        "          # f\"z_where_{d}_{t}\", dist.Normal(z_where_d_t, scale).to_event(1)\n",
        "          f\"z_where_{d}_{t}\", dist.TruncatedNormal(z_where_d_t, scale, low=-1.2, high=1.2).to_event(1)\n",
        "      )\n",
        "      # print(\"z_where_d_t target\", z_where_d_t[0, 0, 0])\n",
        "      z_where_d.append(z_where_d_t)\n",
        "    z_where_d = jnp.stack(z_where_d, -2)\n",
        "    z_where.append(z_where_d)\n",
        "  z_where = jnp.stack(z_where, -3)\n",
        "\n",
        "  worm_frames = jax.lax.stop_gradient(network.decode_what(worms))\n",
        "  worm_frames = resize_batch(worm_frames, 14)\n",
        "\n",
        "  z_what = []\n",
        "  for t in range(T):\n",
        "    z_what_t_mean = worm_frames[..., t, :, :].reshape(worm_frames.shape[:-3] + (14*14,))\n",
        "    z_what_t = numpyro.sample(\n",
        "        f\"z_what_{t}\", dist.Normal(z_what_t_mean, 0.002).to_event(2)\n",
        "        # f\"z_what_{t}\", dist.Delta(z_what_t_mean).to_event(2)\n",
        "    )\n",
        "    # print(\"z_what_t target\", z_what_t[0, 0, 0])\n",
        "    z_what.append(z_what_t)\n",
        "  z_what = jnp.stack(z_what, -2)\n",
        "  \n",
        "  worm_frames = resize_batch(z_what.reshape(z_what.shape[:-1] + (14, 14)), 28)\n",
        "  # worm_frames = network.decode_what(z_what.reshape(z_what.shape[:-1] + (6, 2)))\n",
        "  \n",
        "  # worm_frames = network.decode_what(z_what)\n",
        "  # print(\"worm_frames.shape target\", worm_frames.shape)\n",
        "\n",
        "  # print(\"z_where target\", z_where.shape)\n",
        "  p = embed_frames(worm_frames, z_where, network.frame_size)\n",
        "  # print(\"p.shape target\", p.shape)\n",
        "  p = dist.util.clamp_probs(p.sum(-4))  # sum across worm_frames\n",
        "  # print(\"summed p.shape target\", p.shape)\n",
        "  # print(\"inputs.shape\", inputs.shape)\n",
        "  if sleep_phase:\n",
        "    frames = numpyro.sample(\"frames\", dist.Bernoulli(p).to_event(3))\n",
        "  else:\n",
        "    frames = numpyro.sample(\"frames\", dist.Bernoulli(p).to_event(3), obs=inputs)\n",
        "\n",
        "  out = {\n",
        "      \"frames\": frames,\n",
        "      # \"frames_recon\": p,\n",
        "      \"frames_recon\": jax.lax.stop_gradient(p),\n",
        "      \"worms\": worms,\n",
        "      **{f\"z_what_{t}\": z_what[..., t, :] for t in range(T)},\n",
        "      \"worm_frames\": jax.lax.stop_gradient(worm_frames),\n",
        "      \"params\": params,\n",
        "      **{f\"z_where_{t}\": z_where[..., t, :] for t in range(T)},\n",
        "  }\n",
        "  return (out,)\n",
        "\n",
        "\n",
        "def kernel_where(network, inputs, D=2, t=0, T=10, sleep_phase = False):\n",
        "  if not isinstance(inputs, dict):\n",
        "    # print('making inputs')\n",
        "    inputs = {\n",
        "        \"frames\": inputs,\n",
        "        \"worm_frames\": jnp.ones((D, T, 28, 28)),\n",
        "    }\n",
        "\n",
        "  if sleep_phase:\n",
        "    frame = inputs[\"frames_recon\"][..., t, :, :]\n",
        "  else:\n",
        "    frame = inputs[\"frames\"][..., t, :, :]\n",
        "  z_where_t = []\n",
        "\n",
        "  for d in range(D):\n",
        "    # print(inputs[\"worm_frames\"].shape)\n",
        "    worm_frame = inputs[\"worm_frames\"][..., d, t, :, :]\n",
        "    # print(\"worm_frame shape where\", worm_frame.shape)\n",
        "    resized_frame = resize_batch(frame, 32)\n",
        "    loc, scale = network.encode_where(resized_frame)\n",
        "    # print(loc.shape)\n",
        "    if sleep_phase:\n",
        "      z_where_d_t = numpyro.sample(\n",
        "          f\"z_where_{d}_{t}\", dist.Normal(loc, scale).to_event(1),\n",
        "          obs=inputs[f\"z_where_{d}_{t}\"]\n",
        "      )\n",
        "      # print(\"z_where_d_t where\", z_where_d_t[0, 0, 0])\n",
        "    else:\n",
        "      z_where_d_t = numpyro.sample(\n",
        "          f\"z_where_{d}_{t}\", dist.Normal(loc, scale).to_event(1)\n",
        "      )\n",
        "    z_where_t.append(z_where_d_t)\n",
        "    # print(\"worm_frame shape where\", worm_frame.shape)\n",
        "    # print(\"z_where_d_t shape where\", z_where_d_t.shape)\n",
        "    frame_recon = embed_frames(worm_frame, z_where_d_t, network.frame_size)\n",
        "    # print(\"frame_recon shape where\", frame_recon)\n",
        "    frame = frame - frame_recon\n",
        "  z_where_t = jnp.stack(z_where_t, -2)\n",
        "  # print(\"z_where_t.shape where\", z_where_t.shape)\n",
        "  out = {**inputs, **{f\"z_where_{t}\": z_where_t}}\n",
        "  return (out,)\n",
        "\n",
        "\n",
        "def kernel_what(network, inputs, D=2, t=0, sleep_phase = False):\n",
        "  if sleep_phase:\n",
        "    worm_frames = crop_frames(inputs[\"frames_recon\"][..., t, :, :], -2 -inputs[f\"z_where_{t}\"], 28)\n",
        "  else:\n",
        "    worm_frames = crop_frames(inputs[\"frames\"][..., t, :, :], -2 -inputs[f\"z_where_{t}\"], 28)\n",
        "  loc, scale = network.encode_what(worm_frames)\n",
        "\n",
        "  if sleep_phase:\n",
        "    z_what_t = numpyro.sample(f\"z_what_{t}\", dist.Normal(loc, scale).to_event(2), obs=inputs[f\"z_what_{t}\"])\n",
        "    # print(\"z_what_t what\", z_what_t[0, 0, 0])\n",
        "  else:\n",
        "    z_what_t = numpyro.sample(f\"z_what_{t}\", dist.Normal(loc, scale).to_event(2))\n",
        "\n",
        "  out = {**inputs, **{f\"z_what_{t}\": z_what_t}}\n",
        "  return (out,)\n",
        "\n",
        "\n",
        "def kernel_sim(network, inputs, T=10, sleep_phase = False):\n",
        "  z_what = jnp.stack([inputs[f\"z_what_{t}\"] for t in range(T)], -2)\n",
        "\n",
        "  proposed_sim_params = network.encode_sim(z_what)\n",
        "  loc_L, scale_L, loc_A, scale_A, loc_T, scale_T, loc_kw, scale_kw, loc_ku, scale_ku, loc_inc, scale_inc, loc_dr, scale_dr, loc_phase_1, scale_phase_1, loc_phase_2, scale_phase_2, loc_phase_3, scale_phase_3, loc_alpha, scale_alpha = proposed_sim_params\n",
        "\n",
        "  if sleep_phase:\n",
        "    L = numpyro.sample(\"L\", dist.Normal(loc_L, scale_L).to_event(1), obs=inputs[\"L\"])\n",
        "    # print(\"L sim\", L[0, 0])\n",
        "    A = numpyro.sample(\"A\", dist.Normal(loc_A, scale_A).to_event(1), obs=inputs[\"A\"])\n",
        "    T = numpyro.sample(\"T\", dist.Normal(loc_T, scale_T).to_event(1), obs=inputs[\"T\"])\n",
        "    kw = numpyro.sample(\"kw\", dist.Normal(loc_kw, scale_kw).to_event(1), obs=inputs[\"kw\"])\n",
        "    ku = numpyro.sample(\"ku\", dist.Normal(loc_ku, scale_ku).to_event(1), obs=inputs[\"ku\"])\n",
        "    inc = numpyro.sample(\"inc\", dist.Normal(loc_inc, scale_inc).to_event(1), obs=inputs[\"inc\"])\n",
        "    dr = numpyro.sample(\"dr\", dist.Normal(loc_dr, scale_dr).to_event(1), obs=inputs[\"dr\"])\n",
        "    phase_1 = numpyro.sample(\"phase_1\", dist.Normal(loc_phase_1, scale_phase_1).to_event(1), obs=inputs[\"phase_1\"])\n",
        "    phase_2 = numpyro.sample(\"phase_2\", dist.Normal(loc_phase_2, scale_phase_2).to_event(1), obs=inputs[\"phase_2\"])\n",
        "    phase_3 = numpyro.sample(\"phase_3\", dist.Normal(loc_phase_3, scale_phase_3).to_event(1), obs=inputs[\"phase_3\"])\n",
        "    alpha = numpyro.sample(\"alpha\", dist.Normal(loc_alpha, scale_alpha).to_event(1), obs=inputs[\"alpha\"])\n",
        "  else:\n",
        "    L = numpyro.sample('L', dist.TruncatedNormal(loc_L, scale_L, low=23, high=28).to_event(1))\n",
        "    A = numpyro.sample('A', dist.Normal(loc_A, scale_A).to_event(1))\n",
        "    T = numpyro.sample('T', dist.Normal(loc_T, scale_T).to_event(1))\n",
        "    kw = numpyro.sample('kw', dist.TruncatedNormal(loc_kw, scale_kw, low=0, high=2 * jnp.pi).to_event(1))\n",
        "    ku = numpyro.sample('ku', dist.Normal(loc_ku, scale_ku).to_event(1))\n",
        "    inc = numpyro.sample('inc', dist.TruncatedNormal(loc_inc, scale_inc, low=0, high=2 * jnp.pi).to_event(1))\n",
        "    dr = numpyro.sample('dr', dist.TruncatedNormal(loc_dr, scale_dr, low=0.2, high=0.8).to_event(1))\n",
        "    phase_1 = numpyro.sample('phase_1', dist.TruncatedNormal(loc_phase_1, scale_phase_1, low=0, high=2 * jnp.pi).to_event(1))\n",
        "    phase_2 = numpyro.sample('phase_2', dist.TruncatedNormal(loc_phase_2, scale_phase_2, low=0, high=2 * jnp.pi).to_event(1))\n",
        "    phase_3 = numpyro.sample('phase_3', dist.Normal(loc_phase_3, scale_phase_3).to_event(1))\n",
        "    alpha = numpyro.sample('alpha', dist.Normal(loc_alpha, scale_alpha).to_event(1))\n",
        "\n",
        "  out = {**inputs, **{'L': L, 'A': A, 'T': T, 'kw': kw, 'ku': ku, 'inc': inc, 'dr': dr, 'phase_1': phase_1, 'phase_2': phase_2, 'phase_3': phase_3, 'alpha': alpha}}\n",
        "  return (out,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we create the inference program, define the loss function,\n",
        "run the training loop, and plot the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Args(argparse.Namespace):\n",
        "  batch_size = 512\n",
        "  # batch_size = 16\n",
        "  num_sweeps = 5\n",
        "  num_particles = 1\n",
        "  # learning_rate = 3e-4\n",
        "  # learning_rate = 1e-4\n",
        "  learning_rate = 1e-5\n",
        "  # num_steps = 600_000\n",
        "  # num_steps = 10\n",
        "  num_steps = 100_000\n",
        "  # num_steps = 4000\n",
        "  # num_steps = 2000\n",
        "  device = \"gpu\"\n",
        "\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1724137569.560509    3522 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-08-20 09:06:09.649430: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2024-08-20 09:06:10.301502: W external/xla/xla/service/gpu/nvptx_compiler.cc:836] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version (12.5.82). Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
          ]
        }
      ],
      "source": [
        "lr = args.learning_rate\n",
        "num_steps = args.num_steps\n",
        "batch_size = args.batch_size\n",
        "num_sweeps = args.num_sweeps\n",
        "num_particles = args.num_particles\n",
        "\n",
        "train_ds = load_dataset(is_training=True, batch_size=batch_size, n_data=-1)\n",
        "\n",
        "test_ds = load_dataset(is_training=False, batch_size=batch_size)\n",
        "test_data = next(test_ds)\n",
        "frame_size = test_data.shape[-1]\n",
        "wormsim_net = wormsimAutoEncoder(num_particles=num_particles, batch_size=batch_size, frame_size=frame_size)\n",
        "init_params = wormsim_net.init(jax.random.PRNGKey(0), test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_proposal(network, make_particle_plate, T=10, sleep_phase=False):\n",
        "  kernels = []\n",
        "  for t in range(T):\n",
        "    kernels.append(\n",
        "        make_particle_plate()(partial(kernel_where, network, D=2, t=t, sleep_phase=sleep_phase))\n",
        "    )\n",
        "    kernels.append(make_particle_plate()(partial(kernel_what, network, D=2, t=t, sleep_phase=sleep_phase))\n",
        "    )\n",
        "  kernels.append(make_particle_plate()(partial(kernel_sim, network, T=T, sleep_phase=sleep_phase)))\n",
        "\n",
        "  kernels = [detach(k) for k in kernels]\n",
        "  q = reduce(lambda a, b: compose(b, a), kernels[1:], kernels[0])\n",
        "  return q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def loss_fn(params, key, batch, wormsim_net, num_particles):\n",
        "def loss_fn(params, key, wormsim_net, num_particles, batch_size):\n",
        "    D = 2\n",
        "    T = 10\n",
        "    network = coix.util.BindModule(wormsim_net, params)\n",
        "    make_particle_plate = lambda: numpyro.plate(\"particle\", num_particles, dim=-2)\n",
        "    make_batch_plate = lambda: numpyro.plate(\"batch\", batch_size, dim=-1)\n",
        "    target = make_particle_plate()(make_batch_plate()(partial(wormsim_target, network, D=D, T=T)))\n",
        "\n",
        "    shuffle_rng, rng_key = random.split(key)\n",
        "    # sample from the target\n",
        "    out_model, tr_model, _ = coix.traced_evaluate(target, seed=jax.random.PRNGKey(rng_key[0]))(\n",
        "        None, sleep_phase=True\n",
        "    )\n",
        "    model_sample = {k: v[\"value\"] for k, v in tr_model.items()}\n",
        "\n",
        "    proposal_io = {**out_model[0], **model_sample}\n",
        "\n",
        "    q = make_proposal(network, make_particle_plate, T=T, sleep_phase=True)\n",
        "\n",
        "    out_q, tr_q, metrics = coix.traced_evaluate(q, seed=jax.random.PRNGKey(rng_key[1]))(proposal_io, sleep_phase=True)\n",
        "    q_log_probs = {\n",
        "        name: util.get_site_log_prob(site) for name, site in tr_q.items()\n",
        "    }\n",
        "    # print({k: v[\"is_observed\"] for k, v in tr_q.items()})\n",
        "    # print(model_sample['z_what_0'][0, 0, 0])\n",
        "    # print(tr_q['z_what_0']['value'][0, 0, 0])\n",
        "    # print(out_model[0]['worm_frames'].shape)\n",
        "    # plt.figure()\n",
        "    # plt.imshow(out_model[0]['worm_frames'][0, 0, 0, 0])\n",
        "    # plt.figure()\n",
        "    # plt.imshow(out_model[0]['frames_recon'][0, 0, 0])\n",
        "\n",
        "    # print(model_sample[f\"z_what_{0}\"][0, 0, 0][:5])\n",
        "    # print(tr_q[f\"z_what_{0}\"]['value'][0, 0, 0][:5])\n",
        "    q_log_density = sum(q_log_probs.values()).mean()\n",
        "    z_what_loss = sum(\n",
        "        q_log_probs[f\"z_what_{t}\"]\n",
        "        for t in range(T)\n",
        "    ).mean()\n",
        "    z_where_loss = sum(\n",
        "        q_log_probs[f\"z_where_{d}_{t}\"]\n",
        "        for d in range(D)\n",
        "        for t in range(T)\n",
        "    ).mean()\n",
        "    sim_loss = q_log_probs[\"L\"] + q_log_probs[\"A\"] + q_log_probs[\"T\"] + q_log_probs[\"kw\"] + q_log_probs[\"ku\"] + q_log_probs[\"inc\"] + q_log_probs[\"dr\"] + q_log_probs[\"phase_1\"] + q_log_probs[\"phase_2\"] + q_log_probs[\"phase_3\"] + q_log_probs[\"alpha\"]\n",
        "    metrics['z_what_loss'] = -z_what_loss\n",
        "    metrics['z_where_loss'] = -z_where_loss\n",
        "    metrics['sim_loss'] = -sim_loss.mean()\n",
        "    metrics['loss'] = -q_log_density\n",
        "    return -q_log_density, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# foo, bar = loss_fn(init_params, jax.random.PRNGKey(0), wormsim_net, num_particles, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr_schedule = cosine_decay_schedule(lr, num_steps, 0.5)\n",
        "# lr_schedule = cosine_decay_schedule(lr, num_steps, 1.0)\n",
        "\n",
        "opt = optax.chain(\n",
        "    clip_by_global_norm(1.0),\n",
        "    optax.adam(lr_schedule),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# wormsim_params = np.load(\"worm_learned_params_2.npy\", allow_pickle=True).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# eval_fn(step, params, opt_state, metrics)\n",
        "\n",
        "def eval_fn(step, params, opt_state, metrics):\n",
        "    # print all params norms using tree map\n",
        "    pprint(jax.tree.map(lambda x: jnp.linalg.norm(x), params))\n",
        "    if 'param_norms' in metrics:\n",
        "        metrics['param_norms'].append(jax.tree_map(lambda x: jnp.linalg.norm(x), params))\n",
        "    else:\n",
        "        metrics['params_norms'] = []\n",
        "    # print all metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compiling the first train step...\n",
            "Time to compile a train step: 59.00938820838928\n",
            "=====\n",
            "Step 500   | loss -2895.7471 | sim_loss    63.0766 | squared_grad_norm 94925136.0000 | z_what_loss -3001.7188 | z_where_loss    42.8951\n",
            "Step 1000  | loss -6879.6543 | sim_loss    53.8663 | squared_grad_norm 393452800.0000 | z_what_loss -6956.6865 | z_where_loss    23.1656\n",
            "Step 1500  | loss -8927.9932 | sim_loss    48.0507 | squared_grad_norm 219657504.0000 | z_what_loss -8997.5273 | z_where_loss    21.4839\n",
            "Step 2000  | loss -10510.5361 | sim_loss    45.0470 | squared_grad_norm 41074640.0000 | z_what_loss -10575.5557 | z_where_loss    19.9720\n",
            "Step 2500  | loss -11732.7568 | sim_loss    38.7730 | squared_grad_norm 18305044.0000 | z_what_loss -11790.9033 | z_where_loss    19.3745\n",
            "Step 3000  | loss -12232.3496 | sim_loss    35.5017 | squared_grad_norm 53974852.0000 | z_what_loss -12286.3916 | z_where_loss    18.5402\n",
            "Step 3500  | loss -12391.7695 | sim_loss    31.4405 | squared_grad_norm 64144868.0000 | z_what_loss -12440.0459 | z_where_loss    16.8349\n",
            "Step 4000  | loss -12699.0977 | sim_loss    28.9160 | squared_grad_norm 37518008.0000 | z_what_loss -12743.9541 | z_where_loss    15.9411\n",
            "Step 4500  | loss -12944.3730 | sim_loss    26.5720 | squared_grad_norm 32868832.0000 | z_what_loss -12987.7793 | z_where_loss    16.8338\n",
            "Step 5000  | loss -13070.4541 | sim_loss    24.9519 | squared_grad_norm 37066976.0000 | z_what_loss -13111.4609 | z_where_loss    16.0549\n",
            "Step 5500  | loss -13349.7031 | sim_loss    23.2761 | squared_grad_norm 33882744.0000 | z_what_loss -13388.1367 | z_where_loss    15.1583\n",
            "Step 6000  | loss -13573.0107 | sim_loss    22.8579 | squared_grad_norm 19443628.0000 | z_what_loss -13610.8545 | z_where_loss    14.9866\n",
            "Step 6500  | loss -13883.8223 | sim_loss    22.2572 | squared_grad_norm 12698423.0000 | z_what_loss -13921.0449 | z_where_loss    14.9640\n",
            "Step 7000  | loss -14058.0840 | sim_loss    21.8493 | squared_grad_norm 10389970.0000 | z_what_loss -14094.3906 | z_where_loss    14.4567\n",
            "Step 7500  | loss -14180.4736 | sim_loss    21.8258 | squared_grad_norm 21449898.0000 | z_what_loss -14218.5254 | z_where_loss    16.2259\n",
            "Step 8000  | loss -14200.7197 | sim_loss    21.6407 | squared_grad_norm 21337234.0000 | z_what_loss -14236.4033 | z_where_loss    14.0421\n",
            "Step 8500  | loss -14195.4268 | sim_loss    21.2190 | squared_grad_norm 10937385.0000 | z_what_loss -14229.2568 | z_where_loss    12.6096\n",
            "Step 9000  | loss -14338.8867 | sim_loss    21.2900 | squared_grad_norm 16282851.0000 | z_what_loss -14373.2266 | z_where_loss    13.0511\n",
            "Step 9500  | loss -14383.9004 | sim_loss    21.0979 | squared_grad_norm 15718225.0000 | z_what_loss -14418.0059 | z_where_loss    13.0067\n",
            "Step 10000 | loss -14340.6289 | sim_loss    20.7369 | squared_grad_norm 17139204.0000 | z_what_loss -14373.1279 | z_where_loss    11.7613\n",
            "Step 10500 | loss -14431.1484 | sim_loss    20.6983 | squared_grad_norm 14976504.0000 | z_what_loss -14462.4014 | z_where_loss    10.5540\n",
            "Step 11000 | loss -14348.0801 | sim_loss    20.3183 | squared_grad_norm 36248764.0000 | z_what_loss -14380.5732 | z_where_loss    12.1745\n",
            "Step 11500 | loss -14487.3574 | sim_loss    20.1490 | squared_grad_norm 6982738.0000 | z_what_loss -14518.9893 | z_where_loss    11.4822\n",
            "Step 12000 | loss -14447.5059 | sim_loss    20.1882 | squared_grad_norm 12687083.0000 | z_what_loss -14480.2266 | z_where_loss    12.5335\n",
            "Step 12500 | loss -14307.4385 | sim_loss    19.9166 | squared_grad_norm 67528456.0000 | z_what_loss -14336.5674 | z_where_loss     9.2120\n",
            "Step 13000 | loss -14378.5234 | sim_loss    19.5647 | squared_grad_norm 13989722.0000 | z_what_loss -14408.4492 | z_where_loss    10.3614\n",
            "Step 13500 | loss -14484.0342 | sim_loss    19.5562 | squared_grad_norm 18279918.0000 | z_what_loss -14514.6289 | z_where_loss    11.0388\n",
            "Step 14000 | loss -14602.3721 | sim_loss    19.4522 | squared_grad_norm 13671632.0000 | z_what_loss -14633.1152 | z_where_loss    11.2893\n",
            "Step 14500 | loss -14542.9355 | sim_loss    19.2508 | squared_grad_norm 18482988.0000 | z_what_loss -14570.9258 | z_where_loss     8.7386\n",
            "Step 15000 | loss -14560.2266 | sim_loss    18.7335 | squared_grad_norm 8606343.0000 | z_what_loss -14590.1377 | z_where_loss    11.1776\n",
            "Step 15500 | loss -14572.6143 | sim_loss    18.5505 | squared_grad_norm 30355928.0000 | z_what_loss -14602.2676 | z_where_loss    11.1036\n",
            "Step 16000 | loss -14553.5918 | sim_loss    18.7006 | squared_grad_norm 11609009.0000 | z_what_loss -14582.2578 | z_where_loss     9.9660\n",
            "Step 16500 | loss -14571.2490 | sim_loss    18.8397 | squared_grad_norm 18035024.0000 | z_what_loss -14599.7725 | z_where_loss     9.6839\n",
            "Step 17000 | loss -14563.3555 | sim_loss    18.4540 | squared_grad_norm 37121036.0000 | z_what_loss -14590.8730 | z_where_loss     9.0637\n",
            "Step 17500 | loss -14545.4141 | sim_loss    18.2977 | squared_grad_norm 18950838.0000 | z_what_loss -14572.9951 | z_where_loss     9.2840\n",
            "Step 18000 | loss -14577.2842 | sim_loss    18.4382 | squared_grad_norm 9996734.0000 | z_what_loss -14603.7871 | z_where_loss     8.0642\n",
            "Step 18500 | loss -14624.2451 | sim_loss    17.7350 | squared_grad_norm 6698956.5000 | z_what_loss -14652.4912 | z_where_loss    10.5108\n",
            "Step 19000 | loss -14583.9141 | sim_loss    17.6885 | squared_grad_norm 5731999.5000 | z_what_loss -14609.9199 | z_where_loss     8.3166\n",
            "Step 19500 | loss -14331.0811 | sim_loss    17.6160 | squared_grad_norm 335911776.0000 | z_what_loss -14357.6191 | z_where_loss     8.9220\n",
            "Step 20000 | loss -14578.4668 | sim_loss    17.4231 | squared_grad_norm 8478938.0000 | z_what_loss -14603.9365 | z_where_loss     8.0476\n",
            "Step 20500 | loss -14659.2266 | sim_loss    17.5914 | squared_grad_norm 16467617.0000 | z_what_loss -14686.5781 | z_where_loss     9.7601\n",
            "Step 21000 | loss -14375.2734 | sim_loss    17.3326 | squared_grad_norm 327259936.0000 | z_what_loss -14400.1133 | z_where_loss     7.5079\n",
            "Step 21500 | loss -14665.9580 | sim_loss    17.0903 | squared_grad_norm 27591354.0000 | z_what_loss -14693.2109 | z_where_loss    10.1630\n",
            "Step 22000 | loss -14647.1709 | sim_loss    16.9858 | squared_grad_norm 41034620.0000 | z_what_loss -14673.6992 | z_where_loss     9.5431\n",
            "Step 22500 | loss -14667.1699 | sim_loss    16.4900 | squared_grad_norm 16249277.0000 | z_what_loss -14692.5625 | z_where_loss     8.9021\n",
            "Step 23000 | loss -14663.7139 | sim_loss    16.1118 | squared_grad_norm 13711894.0000 | z_what_loss -14690.0967 | z_where_loss    10.2711\n",
            "Step 23500 | loss -14537.8340 | sim_loss    16.4937 | squared_grad_norm 182446912.0000 | z_what_loss -14564.1777 | z_where_loss     9.8488\n",
            "Step 24000 | loss -14726.8184 | sim_loss    16.7759 | squared_grad_norm 10397078.0000 | z_what_loss -14754.4121 | z_where_loss    10.8166\n",
            "Step 24500 | loss -14673.9395 | sim_loss    16.4940 | squared_grad_norm 11122301.0000 | z_what_loss -14698.7998 | z_where_loss     8.3662\n",
            "Step 25000 | loss -14718.6055 | sim_loss    16.2317 | squared_grad_norm 8048219.0000 | z_what_loss -14742.9648 | z_where_loss     8.1281\n",
            "Step 25500 | loss -14683.5586 | sim_loss    16.0008 | squared_grad_norm 11850425.0000 | z_what_loss -14709.0273 | z_where_loss     9.4673\n",
            "Step 26000 | loss -14660.5664 | sim_loss    15.8718 | squared_grad_norm 20059066.0000 | z_what_loss -14684.6123 | z_where_loss     8.1751\n",
            "Step 26500 | loss -14697.3936 | sim_loss    15.5939 | squared_grad_norm 8093715.5000 | z_what_loss -14721.8262 | z_where_loss     8.8394\n",
            "Step 27000 | loss -14636.6455 | sim_loss    15.3709 | squared_grad_norm 17878666.0000 | z_what_loss -14659.5234 | z_where_loss     7.5073\n",
            "Step 27500 | loss -14670.1172 | sim_loss    15.8045 | squared_grad_norm 34775120.0000 | z_what_loss -14694.8984 | z_where_loss     8.9764\n",
            "Step 28000 | loss -14711.9160 | sim_loss    15.4496 | squared_grad_norm 7128565.5000 | z_what_loss -14735.6670 | z_where_loss     8.3008\n",
            "Step 28500 | loss -14752.8477 | sim_loss    15.1733 | squared_grad_norm 10299253.0000 | z_what_loss -14778.7949 | z_where_loss    10.7745\n",
            "Step 29000 | loss -14743.8232 | sim_loss    15.3975 | squared_grad_norm 18336578.0000 | z_what_loss -14768.2012 | z_where_loss     8.9807\n",
            "Step 29500 | loss -14746.9336 | sim_loss    15.2910 | squared_grad_norm 7743697.5000 | z_what_loss -14771.0234 | z_where_loss     8.7986\n",
            "Step 30000 | loss -14802.3047 | sim_loss    15.4088 | squared_grad_norm 31048556.0000 | z_what_loss -14826.7900 | z_where_loss     9.0757\n",
            "Step 30500 | loss -14743.3652 | sim_loss    15.2074 | squared_grad_norm 16127547.0000 | z_what_loss -14767.1934 | z_where_loss     8.6201\n",
            "Step 31000 | loss -14702.2129 | sim_loss    15.1845 | squared_grad_norm 32852178.0000 | z_what_loss -14726.3838 | z_where_loss     8.9861\n",
            "Step 31500 | loss -14742.4092 | sim_loss    14.7427 | squared_grad_norm 9129766.0000 | z_what_loss -14763.8818 | z_where_loss     6.7301\n",
            "Step 32000 | loss -14788.3135 | sim_loss    14.7481 | squared_grad_norm 12586373.0000 | z_what_loss -14812.0469 | z_where_loss     8.9848\n",
            "Step 32500 | loss -14779.3535 | sim_loss    14.8004 | squared_grad_norm 11183258.0000 | z_what_loss -14803.5840 | z_where_loss     9.4308\n",
            "Step 33000 | loss -14798.8496 | sim_loss    14.5998 | squared_grad_norm 8421175.0000 | z_what_loss -14822.6084 | z_where_loss     9.1592\n",
            "Step 33500 | loss -14854.2871 | sim_loss    14.9623 | squared_grad_norm 9421827.0000 | z_what_loss -14878.8848 | z_where_loss     9.6355\n",
            "Step 34000 | loss -14695.1641 | sim_loss    14.6427 | squared_grad_norm 57175212.0000 | z_what_loss -14716.7910 | z_where_loss     6.9835\n",
            "Step 34500 | loss -14818.0400 | sim_loss    14.4387 | squared_grad_norm 20591188.0000 | z_what_loss -14841.0820 | z_where_loss     8.6032\n",
            "Step 35000 | loss -14729.5781 | sim_loss    14.4984 | squared_grad_norm 113559840.0000 | z_what_loss -14753.0469 | z_where_loss     8.9698\n",
            "Step 35500 | loss -14724.5879 | sim_loss    14.5934 | squared_grad_norm 30305836.0000 | z_what_loss -14747.9209 | z_where_loss     8.7397\n",
            "Step 36000 | loss -14803.9072 | sim_loss    14.5804 | squared_grad_norm 40909696.0000 | z_what_loss -14827.0645 | z_where_loss     8.5772\n",
            "Step 36500 | loss -14891.3271 | sim_loss    14.6379 | squared_grad_norm 4879551.0000 | z_what_loss -14916.8701 | z_where_loss    10.9046\n",
            "Step 37000 | loss -14872.3516 | sim_loss    14.5135 | squared_grad_norm 5171398.5000 | z_what_loss -14896.0332 | z_where_loss     9.1682\n",
            "Step 37500 | loss -14870.4121 | sim_loss    14.5682 | squared_grad_norm 10122335.0000 | z_what_loss -14896.1641 | z_where_loss    11.1836\n",
            "Step 38000 | loss -14820.1914 | sim_loss    14.6743 | squared_grad_norm 94919128.0000 | z_what_loss -14842.8223 | z_where_loss     7.9563\n",
            "Step 38500 | loss -14802.5225 | sim_loss    14.3397 | squared_grad_norm 11861139.0000 | z_what_loss -14824.7500 | z_where_loss     7.8877\n",
            "Step 39000 | loss -14877.2891 | sim_loss    13.7270 | squared_grad_norm 15886650.0000 | z_what_loss -14900.5381 | z_where_loss     9.5223\n",
            "Step 39500 | loss -14880.9521 | sim_loss    14.0995 | squared_grad_norm 22622128.0000 | z_what_loss -14904.4512 | z_where_loss     9.3981\n",
            "Step 40000 | loss -14805.5156 | sim_loss    13.5837 | squared_grad_norm 26521502.0000 | z_what_loss -14827.8008 | z_where_loss     8.7006\n",
            "Step 40500 | loss -14837.9082 | sim_loss    13.5651 | squared_grad_norm 15705894.0000 | z_what_loss -14860.5996 | z_where_loss     9.1262\n",
            "Step 41000 | loss -14851.5791 | sim_loss    13.3600 | squared_grad_norm 19718106.0000 | z_what_loss -14873.3359 | z_where_loss     8.3975\n",
            "Step 41500 | loss -14879.8535 | sim_loss    13.6722 | squared_grad_norm 17518174.0000 | z_what_loss -14901.8223 | z_where_loss     8.2961\n",
            "Step 42000 | loss -14837.0400 | sim_loss    13.7711 | squared_grad_norm 12866139.0000 | z_what_loss -14859.1299 | z_where_loss     8.3190\n",
            "Step 42500 | loss -14883.0059 | sim_loss    13.6050 | squared_grad_norm 8326170.0000 | z_what_loss -14903.8887 | z_where_loss     7.2770\n",
            "Step 43000 | loss -14888.6816 | sim_loss    13.7904 | squared_grad_norm 7353554.5000 | z_what_loss -14910.5039 | z_where_loss     8.0308\n",
            "Step 43500 | loss -14876.4658 | sim_loss    13.4848 | squared_grad_norm 13459018.0000 | z_what_loss -14897.7617 | z_where_loss     7.8114\n",
            "Step 44000 | loss -14907.8438 | sim_loss    13.5835 | squared_grad_norm 44129072.0000 | z_what_loss -14930.2646 | z_where_loss     8.8367\n",
            "Step 44500 | loss -14842.0078 | sim_loss    13.1686 | squared_grad_norm 19202588.0000 | z_what_loss -14864.6113 | z_where_loss     9.4346\n",
            "Step 45000 | loss -14904.7715 | sim_loss    13.4438 | squared_grad_norm 6501789.0000 | z_what_loss -14926.2129 | z_where_loss     7.9989\n",
            "Step 45500 | loss -14848.3008 | sim_loss    13.3474 | squared_grad_norm 15395879.0000 | z_what_loss -14869.1250 | z_where_loss     7.4768\n",
            "Step 46000 | loss -14899.1807 | sim_loss    13.9598 | squared_grad_norm 19413942.0000 | z_what_loss -14921.7676 | z_where_loss     8.6262\n",
            "Step 46500 | loss -14869.6465 | sim_loss    13.1716 | squared_grad_norm 15898031.0000 | z_what_loss -14889.3809 | z_where_loss     6.5630\n",
            "Step 47000 | loss -14905.8203 | sim_loss    12.8408 | squared_grad_norm 48953824.0000 | z_what_loss -14928.1387 | z_where_loss     9.4770\n",
            "Step 47500 | loss -14872.8594 | sim_loss    12.7420 | squared_grad_norm 17546716.0000 | z_what_loss -14893.9004 | z_where_loss     8.2982\n",
            "Step 48000 | loss -14870.4072 | sim_loss    12.6826 | squared_grad_norm 82638920.0000 | z_what_loss -14891.4990 | z_where_loss     8.4092\n",
            "Step 48500 | loss -14953.2969 | sim_loss    13.2597 | squared_grad_norm 11489718.0000 | z_what_loss -14974.8887 | z_where_loss     8.3328\n",
            "Step 49000 | loss -14920.0039 | sim_loss    12.7763 | squared_grad_norm 42589308.0000 | z_what_loss -14942.5186 | z_where_loss     9.7398\n",
            "Step 49500 | loss -14920.4961 | sim_loss    13.0286 | squared_grad_norm 54052208.0000 | z_what_loss -14942.8867 | z_where_loss     9.3627\n",
            "Step 50000 | loss -14885.7051 | sim_loss    12.8917 | squared_grad_norm 70567800.0000 | z_what_loss -14907.4785 | z_where_loss     8.8815\n",
            "Step 50500 | loss -14953.9072 | sim_loss    13.2090 | squared_grad_norm 28624962.0000 | z_what_loss -14975.3174 | z_where_loss     8.2008\n",
            "Step 51000 | loss -14951.6680 | sim_loss    13.0020 | squared_grad_norm 14176647.0000 | z_what_loss -14973.9297 | z_where_loss     9.2587\n",
            "Step 51500 | loss -14929.6670 | sim_loss    12.4574 | squared_grad_norm 11818356.0000 | z_what_loss -14951.3594 | z_where_loss     9.2348\n",
            "Step 52000 | loss -14804.8047 | sim_loss    12.8440 | squared_grad_norm 126535328.0000 | z_what_loss -14824.3359 | z_where_loss     6.6867\n",
            "Step 52500 | loss -14994.0557 | sim_loss    12.3499 | squared_grad_norm 9207191.0000 | z_what_loss -15014.7725 | z_where_loss     8.3671\n",
            "Step 53000 | loss -14874.5195 | sim_loss    12.5886 | squared_grad_norm 119853376.0000 | z_what_loss -14893.5703 | z_where_loss     6.4621\n",
            "Step 53500 | loss -14885.9043 | sim_loss    12.2461 | squared_grad_norm 70748032.0000 | z_what_loss -14904.3428 | z_where_loss     6.1929\n",
            "Step 54000 | loss -14892.2031 | sim_loss    12.2511 | squared_grad_norm 24818896.0000 | z_what_loss -14911.8438 | z_where_loss     7.3887\n",
            "Step 54500 | loss -14944.4453 | sim_loss    12.6444 | squared_grad_norm 15388795.0000 | z_what_loss -14965.3223 | z_where_loss     8.2322\n",
            "Step 55000 | loss -14995.2041 | sim_loss    12.1238 | squared_grad_norm 20107814.0000 | z_what_loss -15013.8740 | z_where_loss     6.5461\n",
            "Step 55500 | loss -15016.7871 | sim_loss    12.1592 | squared_grad_norm 9851094.0000 | z_what_loss -15036.4082 | z_where_loss     7.4638\n",
            "Step 56000 | loss -14958.1162 | sim_loss    12.5413 | squared_grad_norm 18308534.0000 | z_what_loss -14977.9209 | z_where_loss     7.2635\n",
            "Step 56500 | loss -14943.9902 | sim_loss    11.9408 | squared_grad_norm 49884880.0000 | z_what_loss -14961.8955 | z_where_loss     5.9646\n",
            "Step 57000 | loss -14814.3447 | sim_loss    12.2626 | squared_grad_norm 492671968.0000 | z_what_loss -14834.9570 | z_where_loss     8.3496\n",
            "Step 57500 | loss -14970.2822 | sim_loss    12.6159 | squared_grad_norm 19614902.0000 | z_what_loss -14990.4346 | z_where_loss     7.5366\n",
            "Step 58000 | loss -14952.4141 | sim_loss    12.3086 | squared_grad_norm 21252548.0000 | z_what_loss -14971.2012 | z_where_loss     6.4785\n",
            "Step 58500 | loss -14890.8916 | sim_loss    12.2530 | squared_grad_norm 68054776.0000 | z_what_loss -14910.2744 | z_where_loss     7.1303\n",
            "Step 59000 | loss -14935.3145 | sim_loss    12.1950 | squared_grad_norm 130674256.0000 | z_what_loss -14955.2695 | z_where_loss     7.7592\n",
            "Step 59500 | loss -14944.0967 | sim_loss    11.7345 | squared_grad_norm 28353666.0000 | z_what_loss -14961.7148 | z_where_loss     5.8831\n",
            "Step 60000 | loss -14894.5566 | sim_loss    12.2168 | squared_grad_norm 110477184.0000 | z_what_loss -14913.4951 | z_where_loss     6.7208\n",
            "Step 60500 | loss -15001.0615 | sim_loss    12.1834 | squared_grad_norm 11357154.0000 | z_what_loss -15020.7090 | z_where_loss     7.4640\n",
            "Step 61000 | loss -14994.6309 | sim_loss    11.8997 | squared_grad_norm 20388494.0000 | z_what_loss -15013.8789 | z_where_loss     7.3487\n",
            "Step 61500 | loss -14985.1758 | sim_loss    11.8564 | squared_grad_norm 24810972.0000 | z_what_loss -15003.0859 | z_where_loss     6.0524\n",
            "Step 62000 | loss -14984.0098 | sim_loss    11.8692 | squared_grad_norm 29968960.0000 | z_what_loss -15003.2129 | z_where_loss     7.3336\n",
            "Step 62500 | loss -14986.5205 | sim_loss    12.0949 | squared_grad_norm 103665544.0000 | z_what_loss -15005.7646 | z_where_loss     7.1496\n",
            "Step 63000 | loss -14955.6543 | sim_loss    12.0108 | squared_grad_norm 47822320.0000 | z_what_loss -14974.5234 | z_where_loss     6.8590\n",
            "Step 63500 | loss -15019.3789 | sim_loss    11.5740 | squared_grad_norm 10533545.0000 | z_what_loss -15037.3965 | z_where_loss     6.4429\n",
            "Step 64000 | loss -15002.1855 | sim_loss    11.8572 | squared_grad_norm 59870048.0000 | z_what_loss -15021.6611 | z_where_loss     7.6186\n",
            "Step 64500 | loss -15063.5234 | sim_loss    11.3552 | squared_grad_norm 14032760.0000 | z_what_loss -15081.9404 | z_where_loss     7.0615\n",
            "Step 65000 | loss -15043.9746 | sim_loss    11.4799 | squared_grad_norm 17128642.0000 | z_what_loss -15062.0137 | z_where_loss     6.5597\n",
            "Step 65500 | loss -15004.6309 | sim_loss    12.1185 | squared_grad_norm 23453798.0000 | z_what_loss -15022.6699 | z_where_loss     5.9200\n",
            "Step 66000 | loss -15145.6836 | sim_loss    11.2775 | squared_grad_norm 17870780.0000 | z_what_loss -15165.7422 | z_where_loss     8.7800\n",
            "Step 66500 | loss -15060.7725 | sim_loss    11.7409 | squared_grad_norm 13687079.0000 | z_what_loss -15080.2090 | z_where_loss     7.6958\n",
            "Step 67000 | loss -14991.1426 | sim_loss    11.8425 | squared_grad_norm 47627296.0000 | z_what_loss -15010.0469 | z_where_loss     7.0618\n",
            "Step 67500 | loss -14881.8115 | sim_loss    11.6014 | squared_grad_norm 687455872.0000 | z_what_loss -14899.9844 | z_where_loss     6.5707\n",
            "Step 68000 | loss -14928.6113 | sim_loss    11.3511 | squared_grad_norm 237304576.0000 | z_what_loss -14946.8809 | z_where_loss     6.9172\n",
            "Step 68500 | loss -14980.4990 | sim_loss    11.0381 | squared_grad_norm 20675596.0000 | z_what_loss -14995.9902 | z_where_loss     4.4534\n",
            "Step 69000 | loss -15053.8027 | sim_loss    11.2904 | squared_grad_norm 46723524.0000 | z_what_loss -15071.9922 | z_where_loss     6.8996\n",
            "Step 69500 | loss -15053.8320 | sim_loss    11.2438 | squared_grad_norm 14992705.0000 | z_what_loss -15072.4131 | z_where_loss     7.3374\n",
            "Step 70000 | loss -15074.3447 | sim_loss    11.0391 | squared_grad_norm 24477590.0000 | z_what_loss -15091.2676 | z_where_loss     5.8839\n",
            "Step 70500 | loss -14983.7148 | sim_loss    11.3899 | squared_grad_norm 28544634.0000 | z_what_loss -15001.1514 | z_where_loss     6.0464\n",
            "Step 71000 | loss -15081.7832 | sim_loss    11.2228 | squared_grad_norm 22308108.0000 | z_what_loss -15099.2617 | z_where_loss     6.2562\n",
            "Step 71500 | loss -14972.3242 | sim_loss    11.2207 | squared_grad_norm 57489396.0000 | z_what_loss -14989.5508 | z_where_loss     6.0052\n",
            "Step 72000 | loss -15009.9932 | sim_loss    11.2746 | squared_grad_norm 21604516.0000 | z_what_loss -15024.8203 | z_where_loss     3.5524\n",
            "Step 72500 | loss -15049.3428 | sim_loss    11.5044 | squared_grad_norm 69425904.0000 | z_what_loss -15066.1709 | z_where_loss     5.3224\n",
            "Step 73000 | loss -14972.2139 | sim_loss    11.0951 | squared_grad_norm 115063296.0000 | z_what_loss -14987.3086 | z_where_loss     3.9989\n",
            "Step 73500 | loss -15097.2803 | sim_loss    11.2059 | squared_grad_norm 9711894.0000 | z_what_loss -15113.3506 | z_where_loss     4.8647\n",
            "Step 74000 | loss -15065.2783 | sim_loss    11.5958 | squared_grad_norm 13875458.0000 | z_what_loss -15080.7646 | z_where_loss     3.8900\n",
            "Step 74500 | loss -15056.0996 | sim_loss    10.6692 | squared_grad_norm 38082768.0000 | z_what_loss -15071.8535 | z_where_loss     5.0839\n",
            "Step 75000 | loss -15053.9531 | sim_loss    10.9199 | squared_grad_norm 13982468.0000 | z_what_loss -15069.9648 | z_where_loss     5.0920\n",
            "Step 75500 | loss -15062.6797 | sim_loss    11.1497 | squared_grad_norm 23769764.0000 | z_what_loss -15077.7656 | z_where_loss     3.9359\n",
            "Step 76000 | loss -15071.6572 | sim_loss    10.7869 | squared_grad_norm 23898834.0000 | z_what_loss -15087.7998 | z_where_loss     5.3553\n",
            "Step 76500 | loss -15065.6914 | sim_loss    10.9755 | squared_grad_norm 9215701.0000 | z_what_loss -15081.6016 | z_where_loss     4.9344\n",
            "Step 77000 | loss -15057.6309 | sim_loss    10.8834 | squared_grad_norm 25414918.0000 | z_what_loss -15072.2002 | z_where_loss     3.6860\n",
            "Step 77500 | loss -15089.9219 | sim_loss    10.9153 | squared_grad_norm 13813315.0000 | z_what_loss -15105.0566 | z_where_loss     4.2202\n",
            "Step 78000 | loss -15070.0068 | sim_loss    11.7492 | squared_grad_norm 27395608.0000 | z_what_loss -15084.4863 | z_where_loss     2.7303\n",
            "Step 78500 | loss -15072.0049 | sim_loss    11.3196 | squared_grad_norm 14554128.0000 | z_what_loss -15087.6592 | z_where_loss     4.3346\n",
            "Step 79000 | loss -15118.5742 | sim_loss    10.7126 | squared_grad_norm 13977864.0000 | z_what_loss -15133.3457 | z_where_loss     4.0588\n",
            "Step 79500 | loss -15050.3418 | sim_loss    11.0242 | squared_grad_norm 20139088.0000 | z_what_loss -15063.8916 | z_where_loss     2.5255\n",
            "Step 80000 | loss -15051.6348 | sim_loss    10.7956 | squared_grad_norm 17439408.0000 | z_what_loss -15065.4727 | z_where_loss     3.0415\n",
            "Step 80500 | loss -15102.0498 | sim_loss    10.5953 | squared_grad_norm 34442688.0000 | z_what_loss -15116.7607 | z_where_loss     4.1168\n",
            "Step 81000 | loss -15039.1680 | sim_loss    11.0457 | squared_grad_norm 16338187.0000 | z_what_loss -15053.8721 | z_where_loss     3.6588\n",
            "Step 81500 | loss -15058.0996 | sim_loss    10.4568 | squared_grad_norm 16584431.0000 | z_what_loss -15071.9111 | z_where_loss     3.3543\n",
            "Step 82000 | loss -15066.0625 | sim_loss    10.3663 | squared_grad_norm 17226698.0000 | z_what_loss -15080.4688 | z_where_loss     4.0387\n",
            "Step 82500 | loss -15071.5459 | sim_loss    10.8199 | squared_grad_norm 24269802.0000 | z_what_loss -15086.0049 | z_where_loss     3.6396\n",
            "Step 83000 | loss -15025.7266 | sim_loss    10.3427 | squared_grad_norm 156402864.0000 | z_what_loss -15040.0889 | z_where_loss     4.0192\n",
            "Step 83500 | loss -15086.1104 | sim_loss    10.5337 | squared_grad_norm 15811115.0000 | z_what_loss -15100.4609 | z_where_loss     3.8162\n",
            "Step 84000 | loss -15120.3633 | sim_loss    10.5318 | squared_grad_norm 9662624.0000 | z_what_loss -15134.2500 | z_where_loss     3.3536\n",
            "Step 84500 | loss -15090.2568 | sim_loss    11.1306 | squared_grad_norm 26467876.0000 | z_what_loss -15103.9395 | z_where_loss     2.5518\n",
            "Step 85000 | loss -15126.5029 | sim_loss    10.4537 | squared_grad_norm 8890461.0000 | z_what_loss -15140.6328 | z_where_loss     3.6762\n",
            "Step 85500 | loss -15051.6553 | sim_loss    11.0705 | squared_grad_norm 31072868.0000 | z_what_loss -15065.2832 | z_where_loss     2.5571\n",
            "Step 86000 | loss -15034.4727 | sim_loss    10.2120 | squared_grad_norm 68994440.0000 | z_what_loss -15047.3311 | z_where_loss     2.6453\n",
            "Step 86500 | loss -15087.5332 | sim_loss    10.2485 | squared_grad_norm 60333588.0000 | z_what_loss -15100.4102 | z_where_loss     2.6273\n",
            "Step 87000 | loss -15034.5244 | sim_loss     9.9147 | squared_grad_norm 122367568.0000 | z_what_loss -15047.8711 | z_where_loss     3.4327\n",
            "Step 87500 | loss -15145.8398 | sim_loss    10.5127 | squared_grad_norm 30675004.0000 | z_what_loss -15159.1250 | z_where_loss     2.7730\n",
            "Step 88000 | loss -15023.4541 | sim_loss    10.3953 | squared_grad_norm 113800648.0000 | z_what_loss -15036.4072 | z_where_loss     2.5585\n",
            "Step 88500 | loss -15087.6602 | sim_loss    10.2653 | squared_grad_norm 50884240.0000 | z_what_loss -15099.7207 | z_where_loss     1.7948\n",
            "Step 89000 | loss -15101.5010 | sim_loss    10.3676 | squared_grad_norm 10053991.0000 | z_what_loss -15113.7295 | z_where_loss     1.8608\n",
            "Step 89500 | loss -15089.4219 | sim_loss    10.5497 | squared_grad_norm 63340220.0000 | z_what_loss -15102.7412 | z_where_loss     2.7687\n",
            "Step 90000 | loss -15187.3418 | sim_loss    10.5603 | squared_grad_norm 61255036.0000 | z_what_loss -15200.0107 | z_where_loss     2.1090\n",
            "Step 90500 | loss -15061.7988 | sim_loss    10.1876 | squared_grad_norm 22050698.0000 | z_what_loss -15073.9404 | z_where_loss     1.9546\n",
            "Step 91000 | loss -15155.4131 | sim_loss    10.5540 | squared_grad_norm 30307580.0000 | z_what_loss -15169.3223 | z_where_loss     3.3559\n",
            "Step 91500 | loss -15130.7148 | sim_loss    10.4729 | squared_grad_norm 18391920.0000 | z_what_loss -15143.6055 | z_where_loss     2.4165\n",
            "Step 92000 | loss -15208.5293 | sim_loss    10.1986 | squared_grad_norm 26615664.0000 | z_what_loss -15222.5664 | z_where_loss     3.8382\n",
            "Step 92500 | loss -15148.8555 | sim_loss    10.2077 | squared_grad_norm 15133309.0000 | z_what_loss -15161.1191 | z_where_loss     2.0556\n",
            "Step 93000 | loss -15091.7588 | sim_loss    10.4903 | squared_grad_norm 28581202.0000 | z_what_loss -15103.2383 | z_where_loss     0.9896\n",
            "Step 93500 | loss -15037.6328 | sim_loss     9.8693 | squared_grad_norm 56010668.0000 | z_what_loss -15049.4609 | z_where_loss     1.9585\n",
            "Step 94000 | loss -15088.3398 | sim_loss    10.0194 | squared_grad_norm 271555392.0000 | z_what_loss -15100.4785 | z_where_loss     2.1178\n",
            "Step 94500 | loss -15058.4248 | sim_loss     9.7553 | squared_grad_norm 228423808.0000 | z_what_loss -15070.6543 | z_where_loss     2.4750\n",
            "Step 95000 | loss -15146.5234 | sim_loss    10.1052 | squared_grad_norm 27065280.0000 | z_what_loss -15158.8730 | z_where_loss     2.2443\n",
            "Step 95500 | loss -15152.6748 | sim_loss    10.1270 | squared_grad_norm 16403612.0000 | z_what_loss -15163.9062 | z_where_loss     1.1038\n",
            "Step 96000 | loss -15045.3418 | sim_loss    10.1253 | squared_grad_norm 213474336.0000 | z_what_loss -15056.9902 | z_where_loss     1.5228\n",
            "Step 96500 | loss -15092.4629 | sim_loss     9.8666 | squared_grad_norm 118807944.0000 | z_what_loss -15104.5156 | z_where_loss     2.1870\n",
            "Step 97000 | loss -15149.3867 | sim_loss     9.8710 | squared_grad_norm 43729112.0000 | z_what_loss -15161.3477 | z_where_loss     2.0903\n",
            "Step 97500 | loss -15047.6387 | sim_loss     9.9652 | squared_grad_norm 234871856.0000 | z_what_loss -15059.1348 | z_where_loss     1.5312\n",
            "Step 98000 | loss -15138.9551 | sim_loss    10.5402 | squared_grad_norm 65939312.0000 | z_what_loss -15152.6768 | z_where_loss     3.1823\n",
            "Step 98500 | loss -15210.2705 | sim_loss     9.6118 | squared_grad_norm 15143114.0000 | z_what_loss -15223.5107 | z_where_loss     3.6289\n",
            "Step 99000 | loss -15157.1680 | sim_loss    10.3267 | squared_grad_norm 26443708.0000 | z_what_loss -15167.4697 | z_where_loss    -0.0248\n",
            "Step 99500 | loss -15082.4971 | sim_loss     9.5813 | squared_grad_norm 46272852.0000 | z_what_loss -15092.8223 | z_where_loss     0.7450\n",
            "Step 100000 | loss -15138.8281 | sim_loss    10.1855 | squared_grad_norm 10714845.0000 | z_what_loss -15150.6406 | z_where_loss     1.6273\n"
          ]
        }
      ],
      "source": [
        "# with jax.disable_jit():\n",
        "#     wormsim_params, metrics = coix.util.train(\n",
        "#         partial(\n",
        "#             loss_fn,\n",
        "#             wormsim_net=wormsim_net,\n",
        "#             num_particles=num_particles,\n",
        "#             batch_size=batch_size,\n",
        "#         ),\n",
        "#         init_params,\n",
        "#         # wormsim_params,\n",
        "#         # optax.adam(lr),\n",
        "#         opt,\n",
        "#         num_steps,\n",
        "#         # train_ds,\n",
        "#         # eval_fn=eval_fn,\n",
        "#         # log_every=500,\n",
        "#         log_every=1,\n",
        "#     )\n",
        "wormsim_params, metrics = coix.util.train(\n",
        "    partial(\n",
        "        loss_fn,\n",
        "        wormsim_net=wormsim_net,\n",
        "        num_particles=num_particles,\n",
        "        batch_size=batch_size,\n",
        "    ),\n",
        "    init_params,\n",
        "    # wormsim_params,\n",
        "    # optax.adam(lr),\n",
        "    opt,\n",
        "    num_steps,\n",
        "    # train_ds,\n",
        "    # eval_fn=eval_fn,\n",
        "    log_every=500,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'log_weight': Array([[16235.73  , 13450.009 , 15345.275 , 15246.135 , 15346.155 ,\n",
              "         15917.817 , 15086.508 , 15195.499 , 15504.081 , 15820.84  ,\n",
              "         15495.644 , 15190.065 , 15321.113 , 16229.481 , 14358.521 ,\n",
              "         15352.582 , 15534.728 , 14983.408 , 15170.08  , 14468.349 ,\n",
              "         14701.885 , 14737.897 , 15138.35  , 14783.071 , 13612.155 ,\n",
              "         15272.027 , 15642.035 , 15000.862 , 14255.952 , 14836.77  ,\n",
              "         15368.696 , 13892.384 , 15364.173 , 14753.091 , 15008.361 ,\n",
              "         15882.181 , 15187.994 , 16397.113 , 14872.16  , 13596.095 ,\n",
              "         16281.949 , 15964.674 , 14809.457 , 13721.913 , 16388.658 ,\n",
              "         15497.266 , 14342.788 , 14867.306 , 15254.673 , 16317.583 ,\n",
              "         13656.761 , 15581.896 , 15601.563 , 15868.098 , 14628.558 ,\n",
              "         15130.712 , 14697.533 , 15355.51  , 15521.2705, 15189.621 ,\n",
              "         15342.772 , 14240.76  , 15370.377 , 15161.456 , 15973.192 ,\n",
              "         15431.007 , 14676.049 , 14293.569 , 15141.481 , 14132.617 ,\n",
              "         14009.597 , 15102.742 , 15211.828 , 15115.439 , 15359.54  ,\n",
              "         15322.556 , 14103.611 , 14052.244 , 15305.517 , 16286.563 ,\n",
              "         14946.482 , 16041.553 , 16280.863 , 15104.51  , 14709.817 ,\n",
              "         15645.071 , 15682.968 , 14449.534 , 15241.428 , 15289.995 ,\n",
              "         15329.223 , 15598.462 , 13894.404 , 15737.947 , 15870.305 ,\n",
              "         15488.956 , 15051.798 , 14693.205 , 15372.968 , 15585.539 ,\n",
              "         15397.284 , 14699.193 , 15447.052 , 15520.573 , 14601.607 ,\n",
              "         16235.402 , 15978.292 , 15568.499 , 16200.774 , 15373.566 ,\n",
              "         14972.993 , 15200.866 , 15163.594 , 15145.915 , 14408.2   ,\n",
              "         13927.802 , 15190.146 , 15800.788 , 15727.548 , 15417.487 ,\n",
              "         15417.127 , 13796.794 , 14998.547 , 14854.583 , 15799.945 ,\n",
              "         14506.582 , 14149.696 , 15427.632 , 14892.173 , 16050.958 ,\n",
              "         14254.455 , 14587.567 , 14628.818 , 14105.475 , 15506.413 ,\n",
              "         15901.444 , 13521.402 , 16388.031 , 15132.495 , 15611.446 ,\n",
              "         14987.975 , 14746.376 , 16127.894 , 14345.138 , 15589.102 ,\n",
              "         15357.892 , 14586.687 , 13797.157 , 16055.006 , 14648.945 ,\n",
              "         16286.351 , 15338.369 , 14353.151 , 15909.63  , 16224.65  ,\n",
              "         15639.361 , 15864.074 , 15240.968 , 15433.844 , 14776.631 ,\n",
              "         15382.894 , 15198.507 , 13679.628 , 13877.269 , 15418.232 ,\n",
              "         14150.954 , 16266.042 , 15225.225 , 15153.3   , 15122.21  ,\n",
              "         14374.217 , 13992.694 , 16235.245 , 15354.621 , 13828.711 ,\n",
              "         15358.121 , 14684.466 , 14289.523 , 15222.286 , 14841.514 ,\n",
              "         14632.595 , 15244.186 , 13402.422 , 15134.506 , 14138.15  ,\n",
              "         15785.936 , 14754.199 , 15044.129 , 15970.352 , 15433.219 ,\n",
              "         15183.401 , 14799.865 , 15678.314 , 14365.636 , 16361.801 ,\n",
              "         15917.947 , 15833.789 , 14186.49  , 15461.198 , 15378.164 ,\n",
              "         14222.006 , 14816.6455, 16160.015 , 15004.798 , 14550.423 ,\n",
              "         15295.319 , 15297.479 , 16220.217 , 15643.744 , 15393.15  ,\n",
              "         15264.652 , 14408.183 , 13939.343 , 13857.284 , 14145.596 ,\n",
              "         14181.336 , 14088.216 , 13672.175 , 14954.39  , 14484.659 ,\n",
              "         15263.833 , 15323.354 , 15290.868 , 13613.748 , 15507.776 ,\n",
              "         16709.047 , 14816.175 , 14892.617 , 14771.947 , 13955.956 ,\n",
              "         15167.118 , 15490.094 , 15277.256 , 16108.315 , 14858.437 ,\n",
              "         15391.923 , 15898.359 , 14974.685 , 15496.832 , 15682.1875,\n",
              "         16064.298 , 15380.8955, 14493.135 , 15957.946 , 15427.936 ,\n",
              "         15477.867 , 14704.651 , 14731.706 , 15648.207 , 13760.605 ,\n",
              "         15357.343 , 16272.658 , 16113.496 , 15237.858 , 13995.098 ,\n",
              "         14567.585 , 16525.19  , 15223.159 , 15660.05  , 13910.557 ,\n",
              "         16581.979 , 15535.076 , 15126.733 , 15245.836 , 14853.163 ,\n",
              "         14746.881 , 15529.936 , 15266.567 , 14951.382 , 15178.504 ,\n",
              "         15323.913 , 15354.093 , 15181.774 , 15276.657 , 14151.801 ,\n",
              "         16130.991 , 14918.742 , 15619.369 , 15377.683 , 15370.192 ,\n",
              "         15805.433 , 15212.149 , 16053.852 , 14370.908 , 15916.755 ,\n",
              "         15146.053 , 15217.928 , 13491.328 , 14005.782 , 15375.673 ,\n",
              "         15522.829 , 15073.194 , 14172.75  , 14546.214 , 14794.358 ,\n",
              "         14339.376 , 15901.258 , 14263.869 , 16062.757 , 15227.695 ,\n",
              "         15528.175 , 14818.369 , 16359.767 , 15076.813 , 14884.821 ,\n",
              "         15012.796 , 15747.091 , 15201.187 , 14553.702 , 15394.667 ,\n",
              "         14041.83  , 15559.319 , 15334.552 , 15985.647 , 15453.704 ,\n",
              "         16001.866 , 15213.439 , 15648.567 , 15350.003 , 13773.216 ,\n",
              "         15045.217 , 15186.1   , 15704.888 , 15348.11  , 15403.58  ,\n",
              "         13511.241 , 16806.764 , 16258.498 , 14489.61  , 14286.961 ,\n",
              "         14501.195 , 15374.305 , 15460.956 , 15399.149 , 16010.033 ,\n",
              "         15429.444 , 15439.325 , 14786.699 , 16230.553 , 15304.942 ,\n",
              "         15198.867 , 15746.602 , 16274.584 , 15409.085 , 15725.446 ,\n",
              "         15350.772 , 15513.372 , 16292.444 , 14757.412 , 15963.987 ,\n",
              "         15503.822 , 14556.738 , 15313.909 , 15655.307 , 16182.233 ,\n",
              "         13759.381 , 15263.97  , 15601.9375, 14108.63  , 15053.513 ,\n",
              "         16090.728 , 15329.197 , 14546.131 , 15324.454 , 16008.546 ,\n",
              "         15372.106 , 14061.149 , 15228.142 , 16262.363 , 14662.629 ,\n",
              "         15512.071 , 15343.639 , 14532.1   , 15041.798 , 15038.22  ,\n",
              "         15494.246 , 14735.492 , 13905.255 , 15291.529 , 15384.129 ,\n",
              "         14356.138 , 13951.355 , 17146.37  , 15522.372 , 15930.882 ,\n",
              "         15129.047 , 14219.676 , 14531.605 , 14470.751 , 15817.526 ,\n",
              "         15532.442 , 15403.446 , 15607.221 , 15875.652 , 16195.9795,\n",
              "         15151.21  , 13762.157 , 13935.561 , 15239.747 , 15432.729 ,\n",
              "         14428.515 , 14190.04  , 14694.994 , 15179.784 , 15492.434 ,\n",
              "         15389.946 , 15840.088 , 15248.414 , 15767.339 , 14512.062 ,\n",
              "         16280.649 , 15685.705 , 14563.538 , 14734.543 , 15086.017 ,\n",
              "         15904.392 , 10396.395 , 14268.796 , 15300.355 , 15510.071 ,\n",
              "         15164.777 , 14167.274 , 16187.661 , 13509.635 , 13584.977 ,\n",
              "         15537.157 , 14333.414 , 13991.169 , 14535.425 , 16107.17  ,\n",
              "         15814.157 , 14024.527 , 13897.861 , 15552.41  , 15097.812 ,\n",
              "         13950.178 , 16087.494 , 16852.957 , 15075.91  , 15390.309 ,\n",
              "         16190.942 , 15400.771 , 16229.538 , 15455.24  , 13845.636 ,\n",
              "         15912.107 , 15400.388 , 15279.724 , 15047.84  , 15389.634 ,\n",
              "         14692.086 , 15551.392 , 14585.623 , 15410.152 , 16003.017 ,\n",
              "         15284.912 , 14947.386 , 15729.906 , 14432.171 , 15296.822 ,\n",
              "         15514.446 , 14908.304 , 15922.103 , 14992.841 , 14678.275 ,\n",
              "         14166.7705, 15940.736 , 15207.568 , 15501.592 , 15108.425 ,\n",
              "         14268.982 , 15627.05  , 15070.701 , 15899.524 , 15563.915 ,\n",
              "         14225.099 , 16025.875 , 13610.948 , 15603.294 , 14900.014 ,\n",
              "         15877.159 , 14295.106 , 17094.68  , 14982.431 , 15304.756 ,\n",
              "         13990.918 , 13989.977 , 15481.565 , 14363.876 , 15641.906 ,\n",
              "         14989.573 , 16432.771 , 15771.07  , 14203.706 , 15336.318 ,\n",
              "         15303.232 , 15063.383 , 15947.104 , 15821.123 , 14583.501 ,\n",
              "         14162.608 , 15081.642 , 14685.683 , 15372.203 , 15189.923 ,\n",
              "         14796.637 , 14232.446 , 15746.198 , 15003.487 , 15266.352 ,\n",
              "         15303.8545, 15198.363 ]], dtype=float32),\n",
              " 'loss': Array(-15138.828, dtype=float32),\n",
              " 'sim_loss': Array(10.185506, dtype=float32),\n",
              " 'squared_grad_norm': Array(10714845., dtype=float32),\n",
              " 'z_what_loss': Array(-15150.641, dtype=float32),\n",
              " 'z_where_loss': Array(1.6272849, dtype=float32)}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.save(\"worm_sleep_phase_learned_params.npy\", wormsim_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "T = 10\n",
        "D = 2\n",
        "network = coix.util.BindModule(wormsim_net, wormsim_params)\n",
        "make_particle_plate = lambda: numpyro.plate(\"particle\", num_particles, dim=-2)\n",
        "make_batch_plate = lambda: numpyro.plate(\"batch\", batch_size, dim=-1)\n",
        "target = make_particle_plate()(make_batch_plate()(partial(wormsim_target, network, D=D, T=T)))\n",
        "shuffle_rng, rng_key = random.split(jax.random.PRNGKey(0))\n",
        "# sample from the target\n",
        "out_model, tr_model, _ = coix.traced_evaluate(target, seed=jax.random.PRNGKey(rng_key[0]))(\n",
        "    None, sleep_phase=False\n",
        ")\n",
        "model_sample = {k: v[\"value\"] for k, v in tr_model.items()}\n",
        "proposal_io = {**out_model[0], **model_sample}\n",
        "q = make_proposal(network, make_particle_plate, T=T)\n",
        "# out_q, tr_q, metrics = coix.traced_evaluate(q, seed=jax.random.PRNGKey(rng_key[1]))(proposal_io, sleep_phase=False)\n",
        "out_q, tr_q, metrics = coix.traced_evaluate(propose(target, q), seed=jax.random.PRNGKey(rng_key[1]))(out_model[0]['frames_recon'], sleep_phase=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['L', 'A', 'T', 'kw', 'ku', 'inc', 'dr', 'phase_1', 'phase_2', 'phase_3', 'alpha', 'z_where_0_0', 'z_where_0_1', 'z_where_0_2', 'z_where_0_3', 'z_where_0_4', 'z_where_0_5', 'z_where_0_6', 'z_where_0_7', 'z_where_0_8', 'z_where_0_9', 'z_where_1_0', 'z_where_1_1', 'z_where_1_2', 'z_where_1_3', 'z_where_1_4', 'z_where_1_5', 'z_where_1_6', 'z_where_1_7', 'z_where_1_8', 'z_where_1_9', 'z_what_0', 'z_what_1', 'z_what_2', 'z_what_3', 'z_what_4', 'z_what_5', 'z_what_6', 'z_what_7', 'z_what_8', 'z_what_9', 'frames'])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tr_q.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 512, 10, 64, 64)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out_model[0]['frames_recon'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7925f33fb3e0>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(out_model[0]['frames_recon'][0, 2, 2])\n",
        "# plt.imshow(out_model[0]['frames'][0, 1, 8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['frames', 'frames_recon', 'worms', 'z_what_0', 'z_what_1', 'z_what_2', 'z_what_3', 'z_what_4', 'z_what_5', 'z_what_6', 'z_what_7', 'z_what_8', 'z_what_9', 'worm_frames', 'params', 'z_where_0', 'z_where_1', 'z_where_2', 'z_where_3', 'z_where_4', 'z_where_5', 'z_where_6', 'z_where_7', 'z_where_8', 'z_where_9'])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out_q[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7925feff7f80>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(out_q[0]['frames_recon'][0, 2, 2])\n",
        "# plt.imshow(out_q[0]['frames'][0, 1, 8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['L', 'A', 'T', 'kw', 'ku', 'inc', 'dr', 'phase_1', 'phase_2', 'phase_3', 'alpha', 'z_where_0_0', 'z_where_0_1', 'z_where_0_2', 'z_where_0_3', 'z_where_0_4', 'z_where_0_5', 'z_where_0_6', 'z_where_0_7', 'z_where_0_8', 'z_where_0_9', 'z_where_1_0', 'z_where_1_1', 'z_where_1_2', 'z_where_1_3', 'z_where_1_4', 'z_where_1_5', 'z_where_1_6', 'z_where_1_7', 'z_where_1_8', 'z_where_1_9', 'z_what_0', 'z_what_1', 'z_what_2', 'z_what_3', 'z_what_4', 'z_what_5', 'z_what_6', 'z_what_7', 'z_what_8', 'z_what_9', 'frames'])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tr_q.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
