{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Simulator-based unsupervised detection and tracking of worms\n",
        "\n",
        "**References**\n",
        "\n",
        "    1. Wu, Hao, et al. Amortized population Gibbs samplers with neural\n",
        "       sufficient statistics. ICML 2020.\n",
        "\n",
        "<img src=\"file://../_static/wormsim.gif\" align=\"center\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/frans/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2024-08-19 21:49:24.323245: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-19 21:49:24.342165: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-19 21:49:24.347660: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-19 21:49:25.285469: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "from functools import partial, reduce\n",
        "\n",
        "import coix\n",
        "from coix.core import detach\n",
        "from coix.api import compose, propose\n",
        "from coix import util\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "from jax import random\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib.patches import Rectangle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpyro\n",
        "import numpyro.distributions as dist\n",
        "import optax\n",
        "from optax import cosine_decay_schedule\n",
        "from optax import clip_by_global_norm\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from sim_utils import *\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def dataset_generator(file_path, n_data=-1):\n",
        "    ds = np.load(file_path, mmap_mode='r')\n",
        "    ds = ds[:n_data] if n_data != -1 else ds\n",
        "    for data in ds:\n",
        "        yield data\n",
        "\n",
        "def load_dataset(*, is_training, batch_size, n_data=-1, file_path=\"worms_train_40k.npy\"):\n",
        "    # Create a dataset from the generator\n",
        "    ds = tf.data.Dataset.from_generator(\n",
        "        dataset_generator,\n",
        "        args=(file_path, n_data),\n",
        "        output_signature=tf.TensorSpec(shape=(None, None, None), dtype=tf.float32)\n",
        "    )\n",
        "    \n",
        "    ds = ds.repeat()\n",
        "    if is_training:\n",
        "        ds = ds.shuffle(10 * batch_size, seed=0)\n",
        "    ds = ds.batch(batch_size)\n",
        "    \n",
        "    # Standardize the data between 0 and 1\n",
        "    ds = ds.map(lambda x: x / 0.80999994)\n",
        "    return iter(tfds.as_numpy(ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vmapped_sim_fn(sim_fn, params):\n",
        "    if params['L'].ndim == 1:\n",
        "        return jax.vmap(sim_fn, in_axes=0, out_axes=0)(params)\n",
        "    else:\n",
        "        return jax.vmap(partial(vmapped_sim_fn, sim_fn), in_axes=0, out_axes=0)(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sim_worms(nworms, n_frames):\n",
        "    duration = 1.55\n",
        "    snapshots = 10\n",
        "    kpoints = 6\n",
        "    L_low = 23\n",
        "    L_high = 28\n",
        "    L = numpyro.sample('L', dist.Uniform(L_low, L_high).expand([nworms]).to_event())\n",
        "    A = numpyro.sample('A', dist.Normal(1, 0.1).expand([nworms]).to_event())\n",
        "    T = numpyro.sample('T', dist.Normal(0.8, 0.1).expand([nworms]).to_event())\n",
        "    kw = numpyro.sample('kw', dist.Uniform(0, 2 * jnp.pi).expand([nworms]).to_event())\n",
        "    ku = numpyro.sample('ku', dist.Normal(jnp.pi, 1).expand([nworms]).to_event())\n",
        "\n",
        "    inc = numpyro.sample('inc', dist.Uniform(0, 2 * jnp.pi).expand([nworms]).to_event())\n",
        "    dr = numpyro.sample('dr', dist.Uniform(0.2, 0.8).expand([nworms]).to_event())\n",
        "    phase_1 = numpyro.sample('phase_1', dist.Uniform(0, 2 * jnp.pi).expand([nworms]).to_event())\n",
        "    phase_2 = numpyro.sample('phase_2', dist.Uniform(0, 2 * jnp.pi).expand([nworms]).to_event())\n",
        "    phase_3 = numpyro.sample('phase_3', dist.Normal(0, 0.1).expand([nworms]).to_event())\n",
        "    alpha = numpyro.sample('alpha', dist.Normal(4, 4).expand([nworms]).to_event())\n",
        "    alpha = jnp.abs(alpha + 1.0)\n",
        "\n",
        "    x0 = jnp.zeros_like(L)\n",
        "    y0 = jnp.zeros_like(L)\n",
        "\n",
        "    params = {'L': L, 'A': A, 'T': T, 'kw': kw, 'ku': ku, 'inc': inc, 'dr': dr, 'phase_1': phase_1, 'phase_2': phase_2, 'phase_3': phase_3, 'alpha': alpha, 'x0': x0, 'y0': y0}\n",
        "    sim_fn = partial(\n",
        "        worm_simulation,\n",
        "        duration=duration,\n",
        "        snapshots=snapshots,\n",
        "        kpoints=kpoints,\n",
        "    )\n",
        "    worms = vmapped_sim_fn(sim_fn, params)\n",
        "    worms = worms / ((L_high + L_low) / 2)\n",
        "    numpyro.deterministic('worms', worms)\n",
        "    return worms, params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def scale_and_translate(image, where, out_size):\n",
        "  translate = abs(image.shape[-1] - out_size) * (where[..., ::-1] + 1) / 2\n",
        "  return jax.image.scale_and_translate(\n",
        "      image,\n",
        "      (out_size, out_size),\n",
        "      (0, 1),\n",
        "      jnp.ones(2),\n",
        "      translate,\n",
        "      method=\"cubic\",\n",
        "      antialias=False,\n",
        "  )\n",
        "\n",
        "def scale_and_translate_variable_scale(image, where, scale, out_size):\n",
        "  translate = abs(image.shape[-1] - out_size) * (where[..., ::-1] + 1) / 2\n",
        "  return jax.image.scale_and_translate(\n",
        "      image,\n",
        "      (out_size, out_size),\n",
        "      (0, 1),\n",
        "      scale * jnp.ones(2),\n",
        "      translate,\n",
        "      method=\"cubic\",\n",
        "      antialias=False,\n",
        "  )\n",
        "\n",
        "def crop_frames(frames, z_where, digit_size=28):\n",
        "  # frames:           time.frame_size.frame_size\n",
        "  # z_where: (worm_frames).time.2\n",
        "  # out:     (digits).time.digit_size.digit_size\n",
        "  if frames.ndim == 2 and z_where.ndim == 1:\n",
        "    return scale_and_translate(frames, z_where, out_size=digit_size)\n",
        "  elif frames.ndim == 3 and z_where.ndim == 2:\n",
        "    in_axes = (0, 0)\n",
        "  elif frames.ndim == 4 and z_where.ndim == 3:\n",
        "    in_axes = (0, 0)\n",
        "  elif frames.ndim == 3 and z_where.ndim == 3:\n",
        "    in_axes = (None, 0)\n",
        "  elif frames.ndim == 2 and z_where.ndim == 2:\n",
        "    in_axes = (None, 0)\n",
        "  elif frames.ndim == z_where.ndim:\n",
        "    in_axes = (0, 0)\n",
        "  elif frames.ndim > z_where.ndim:\n",
        "    in_axes = (0, None)\n",
        "  else:\n",
        "    in_axes = (None, 0)\n",
        "  return jax.vmap(partial(crop_frames, digit_size=digit_size), in_axes)(\n",
        "      frames, z_where\n",
        "  )\n",
        "\n",
        "\n",
        "def embed_frames(worm_frames, z_where, frame_size=64):\n",
        "  # worm_frames:  (worm_frames).      .digit_size.digit_size\n",
        "  # z_where: (worm_frames).(time).2\n",
        "  # out:     (worm_frames).(time).frame_size.frame_size\n",
        "  if worm_frames.ndim == 2 and z_where.ndim == 1:\n",
        "    return scale_and_translate(worm_frames, z_where, out_size=frame_size)\n",
        "  elif worm_frames.ndim == 2 and z_where.ndim == 2:\n",
        "    in_axes = (None, 0)\n",
        "  elif worm_frames.ndim >= z_where.ndim:\n",
        "    in_axes = (0, 0)\n",
        "  else:\n",
        "    in_axes = (None, 0)\n",
        "  return jax.vmap(partial(embed_frames, frame_size=frame_size), in_axes)(\n",
        "      worm_frames, z_where\n",
        "  )\n",
        "\n",
        "def embed_worms(worm_frames, z_where, scale, frame_size=64):\n",
        "  # worm_frames:  (worm_frames).      .digit_size.digit_size\n",
        "  # z_where: (worm_frames).(time).2\n",
        "  # out:     (worm_frames).(time).frame_size.frame_size\n",
        "  if worm_frames.ndim == 2 and z_where.ndim == 1:\n",
        "    return scale_and_translate_variable_scale(worm_frames, z_where, scale, out_size=frame_size)\n",
        "  elif worm_frames.ndim == 2 and z_where.ndim == 2:\n",
        "    in_axes = (None, 0, 0)\n",
        "  elif worm_frames.ndim >= z_where.ndim:\n",
        "    in_axes = (0, 0, 0)\n",
        "  else:\n",
        "    in_axes = (None, 0, 0)\n",
        "  return jax.vmap(partial(embed_worms, frame_size=frame_size), in_axes)(\n",
        "      worm_frames, z_where, scale\n",
        "  )\n",
        "\n",
        "def conv2d(frames, worm_frames):\n",
        "  # frames:          (time).frame_size.frame_size\n",
        "  # worm_frames: (worm_frames).      .digit_size.digit_size\n",
        "  # out:    (worm_frames).(time).conv_size .conv_size\n",
        "  if frames.ndim == 2 and worm_frames.ndim == 2:\n",
        "    return jax.scipy.signal.convolve2d(frames, worm_frames, mode=\"valid\")\n",
        "  elif frames.ndim == worm_frames.ndim:\n",
        "    in_axes = (0, 0)\n",
        "  elif frames.ndim > worm_frames.ndim:\n",
        "    in_axes = (0, None)\n",
        "  else:\n",
        "    in_axes = (None, 0)\n",
        "  return jax.vmap(conv2d, in_axes=in_axes)(frames, worm_frames)\n",
        "\n",
        "def resize_batch(frames, size):\n",
        "  if frames.ndim == 2:\n",
        "    return jax.image.resize(frames, (size, size), method=\"cubic\")\n",
        "  elif frames.ndim > 2:\n",
        "    return jax.vmap(partial(resize_batch, size=size))(frames)\n",
        "\n",
        "# interpolate the sparse points outputted by the simulator\n",
        "def interpolate(worms, n_points=32):\n",
        "  if worms.ndim == 1:\n",
        "    return jnp.interp(jnp.linspace(0, 1, n_points), jnp.linspace(0, 1, worms.shape[0]), worms)\n",
        "  elif worms.ndim == 2:\n",
        "    return jax.vmap(partial(interpolate, n_points=n_points), in_axes=1, out_axes=1)(worms)\n",
        "  else:\n",
        "    return jax.vmap(partial(interpolate, n_points=n_points), in_axes=0)(worms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderSim(nn.Module):\n",
        "  \"\"\"\n",
        "  Takes sequence of z_what and encodes them into a distribution over worm simulator parameters\n",
        "  \"\"\"\n",
        "  @nn.compact\n",
        "  def __call__(self, z_what, carry=None):\n",
        "    broadcast_dims = z_what.shape[:-2]\n",
        "    hidden_dim1 = 512\n",
        "    hidden_dim2 = 256\n",
        "    \n",
        "    # Process input\n",
        "    x = z_what\n",
        "    \n",
        "    # Initialize carry state for the first GRU\n",
        "    if carry is None:\n",
        "        carry = self.param('carry_init', \n",
        "                           lambda rng, shape: jnp.zeros(shape), \n",
        "                           (hidden_dim1,))\n",
        "    \n",
        "    # First GRU Layer\n",
        "    GRU1 = nn.scan(nn.GRUCell,\n",
        "                  in_axes=-2,\n",
        "                  out_axes=-2,\n",
        "                  variable_broadcast='params',\n",
        "                  split_rngs={'params': False}\n",
        "                  )(hidden_dim1)\n",
        "    \n",
        "    # print(\"x.shape\", x.shape)\n",
        "    # Apply LayerNorm and tile the carry state to match the batch size\n",
        "    x = nn.LayerNorm()(x)\n",
        "    carry1 = jnp.tile(carry, broadcast_dims + (1,))\n",
        "    carry1, x = GRU1(carry1, x)\n",
        "    x = nn.LayerNorm()(x)\n",
        "    \n",
        "    # Initialize carry state for the second GRU\n",
        "    carry2 = self.param('carry2_init', \n",
        "                        lambda rng, shape: jnp.zeros(shape), \n",
        "                        (hidden_dim2,))\n",
        "    \n",
        "    # print(\"x.shape\", x.shape)\n",
        "    # Second GRU Layer\n",
        "    GRU2 = nn.scan(nn.GRUCell,\n",
        "                  in_axes=-2,\n",
        "                  out_axes=-2,\n",
        "                  variable_broadcast='params',\n",
        "                  split_rngs={'params': False}\n",
        "                  )(hidden_dim2)\n",
        "    \n",
        "    # Tile the carry state to match the batch size\n",
        "    carry2 = jnp.tile(carry2, broadcast_dims + (1,))\n",
        "    carry2, x = GRU2(carry2, x)\n",
        "\n",
        "    # print(\"x.shape\", x.shape)\n",
        "    # x = x[..., -1, :] # use the last time step\n",
        "    x = x.sum(-2) # sum across time\n",
        "    # print(\"x.shape\", x.shape)\n",
        "    x = nn.Dense(64)(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.LayerNorm()(x)\n",
        "    \n",
        "    x_L = nn.Dense(10)(x)\n",
        "    x_L = nn.relu(x_L)\n",
        "    x_L_loc = nn.Dense(1)(x_L)\n",
        "    # scale to 10-15\n",
        "    x_L_loc = nn.tanh(x_L_loc) * ((28 - 23) / 2) + (23 + 28) / 2\n",
        "    x_L_scale = 0.5 * nn.Dense(1)(x_L)\n",
        "\n",
        "    x_A = nn.Dense(10)(x)\n",
        "    x_A = nn.relu(x_A)\n",
        "    x_A_loc = nn.Dense(1)(x_A)\n",
        "    x_A_scale = 0.5 * nn.Dense(1)(x_A)\n",
        "\n",
        "    x_T = nn.Dense(10)(x)\n",
        "    x_T = nn.relu(x_T)\n",
        "    x_T_loc = nn.Dense(1)(x_T)\n",
        "    # constrain to positive\n",
        "    x_T_loc = nn.softplus(x_T_loc)\n",
        "    x_T_scale = 0.5 * nn.Dense(1)(x_T)\n",
        "\n",
        "    x_kw = nn.Dense(10)(x)\n",
        "    x_kw = nn.relu(x_kw)\n",
        "    x_kw_loc = nn.Dense(1)(x_kw)\n",
        "    # scale to 0-2pi\n",
        "    x_kw_loc = nn.tanh(x_kw_loc) * jnp.pi + jnp.pi\n",
        "    x_kw_scale = 0.5 * nn.Dense(1)(x_kw)\n",
        "\n",
        "    x_ku = nn.Dense(10)(x)\n",
        "    x_ku = nn.relu(x_ku)\n",
        "    x_ku_loc = nn.Dense(1)(x_ku)\n",
        "    x_ku_scale = 0.5 * nn.Dense(1)(x_ku)\n",
        "\n",
        "    x_inc = nn.Dense(10)(x)\n",
        "    x_inc = nn.relu(x_inc)\n",
        "    x_inc_loc = nn.Dense(1)(x_inc)\n",
        "    # scale to 0-2pi\n",
        "    x_inc_loc = nn.tanh(x_inc_loc) * jnp.pi + jnp.pi\n",
        "    x_inc_scale = 0.5 * nn.Dense(1)(x_inc)\n",
        "\n",
        "    x_dr = nn.Dense(10)(x)\n",
        "    x_dr = nn.relu(x_dr)\n",
        "    x_dr_loc = nn.Dense(1)(x_dr)\n",
        "    # scale to 0.2-0.8\n",
        "    x_dr_loc = nn.tanh(x_dr_loc) * 0.3 + 0.5\n",
        "    x_dr_scale = 0.5 * nn.Dense(1)(x_dr)\n",
        "\n",
        "    x_phase_1 = nn.Dense(10)(x)\n",
        "    x_phase_1 = nn.relu(x_phase_1)\n",
        "    x_phase_1_loc = nn.Dense(1)(x_phase_1)\n",
        "    # scale to 0-2pi\n",
        "    x_phase_1_loc = nn.tanh(x_phase_1_loc) * jnp.pi + jnp.pi\n",
        "    x_phase_1_scale = 0.5 * nn.Dense(1)(x_phase_1)\n",
        "\n",
        "    x_phase_2 = nn.Dense(10)(x)\n",
        "    x_phase_2 = nn.relu(x_phase_2)\n",
        "    x_phase_2_loc = nn.Dense(1)(x_phase_2)\n",
        "    # scale to 0-2pi\n",
        "    x_phase_2_loc = nn.tanh(x_phase_2_loc) * jnp.pi + jnp.pi\n",
        "    x_phase_2_scale = 0.5 * nn.Dense(1)(x_phase_2)\n",
        "\n",
        "    x_phase_3 = nn.Dense(10)(x)\n",
        "    x_phase_3 = nn.relu(x_phase_3)\n",
        "    x_phase_3_loc = nn.Dense(1)(x_phase_3)\n",
        "    x_phase_3_scale = 0.5 * nn.Dense(1)(x_phase_3)\n",
        "\n",
        "    x_alpha = nn.Dense(10)(x)\n",
        "    x_alpha = nn.relu(x_alpha)\n",
        "    x_alpha_loc = nn.Dense(1)(x_alpha)\n",
        "    x_alpha_scale = 0.5 * nn.Dense(1)(x_alpha)\n",
        "\n",
        "    return x_L_loc.squeeze(-1), jnp.exp(x_L_scale.squeeze(-1)), x_A_loc.squeeze(-1), jnp.exp(x_A_scale.squeeze(-1)), x_T_loc.squeeze(-1), jnp.exp(x_T_scale.squeeze(-1)), x_kw_loc.squeeze(-1), jnp.exp(x_kw_scale.squeeze(-1)), x_ku_loc.squeeze(-1), jnp.exp(x_ku_scale.squeeze(-1)), x_inc_loc.squeeze(-1), jnp.exp(x_inc_scale.squeeze(-1)), x_dr_loc.squeeze(-1), jnp.exp(x_dr_scale.squeeze(-1)), x_phase_1_loc.squeeze(-1), jnp.exp(x_phase_1_scale.squeeze(-1)), x_phase_2_loc.squeeze(-1), jnp.exp(x_phase_2_scale.squeeze(-1)), x_phase_3_loc.squeeze(-1), jnp.exp(x_phase_3_scale.squeeze(-1)), x_alpha_loc.squeeze(-1), jnp.exp(x_alpha_scale.squeeze(-1))\n",
        "\n",
        "\n",
        "class EncoderWhere(nn.Module):\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, frame_conv):\n",
        "    x = jnp.expand_dims(frame_conv, -1)\n",
        "    x = nn.Conv(features=64, kernel_size=(3, 3), strides=(2, 2))(x)\n",
        "    x = nn.relu(x) \n",
        "    x = nn.Conv(features=32, kernel_size=(3, 3), strides=(2, 2))(x)\n",
        "    x = nn.relu(x) \n",
        "    x = x.reshape(x.shape[:-3] + (-1,))\n",
        "    x = nn.Dense(64)(x)\n",
        "    x = x.reshape(x.shape[:-1] + (2, 32))\n",
        "    x = nn.relu(x)\n",
        "    loc_raw = nn.Dense(2)(x[..., 0, :])\n",
        "    scale_raw = 0.5 * nn.Dense(2)(x[..., 1, :])\n",
        "    return nn.tanh(loc_raw), jnp.exp(scale_raw)\n",
        "\n",
        "class DecoderWhat(nn.Module):\n",
        "  \"\"\"\n",
        "  Hardcoded decoder to plot worm coordinates on a frame\n",
        "  \"\"\"\n",
        "  @nn.compact\n",
        "  def __call__(self, worms):\n",
        "    # vmap interpolate over all worms\n",
        "    worms = interpolate(worms, n_points = 16)    \n",
        "\n",
        "    # make worm widths for all knots\n",
        "    # R = 0.8\n",
        "    R = nn.softplus(nn.Dense(1)(jnp.ones(1))) + 0.7 # we need some params for the module to be registered\n",
        "    # worm_scale = nn.softplus(nn.Dense(1)(jnp.ones(1))) * 2\n",
        "    worm_scale = 2.5\n",
        "    # print(\"R:\", R)\n",
        "    K = worms.shape[-2]\n",
        "    i = jnp.arange(K)\n",
        "    r = R * jnp.abs(jnp.sin(jnp.arccos((i - K / 2) / (K / 2 + 0.2))))\n",
        "    r = jnp.tile(r, (worms.shape[:-2] + (1,)))\n",
        "\n",
        "    # draw the circles\n",
        "    # circles = embed_worms(circle_image(4), worms * worm_scale, r, frame_size=28)\n",
        "    circles = embed_worms(jnp.ones((2, 2)), worms * worm_scale, r, frame_size=28)\n",
        "    # overlay the circles\n",
        "    p = circles.sum(-3)\n",
        "    p = p ** 0.001\n",
        "    p -= p.min()\n",
        "    p = p / p.max()\n",
        "    return p\n",
        "\n",
        "class wormsimAutoEncoder(nn.Module):\n",
        "  num_particles: int\n",
        "  batch_size: int\n",
        "  frame_size: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.encode_sim = EncoderSim()\n",
        "    self.encode_where = EncoderWhere()\n",
        "    self.decode_what = DecoderWhat()\n",
        "\n",
        "  def __call__(self, frames):\n",
        "    \"\"\"\n",
        "    Only used to initialize the model\n",
        "    \"\"\"\n",
        "    T = 10\n",
        "    batch_size = frames.shape[0]\n",
        "    \n",
        "    # print(\"frames.shape\", frames.shape)\n",
        "    resized_frames = resize_batch(frames, 32)\n",
        "    # print(\"resized_frames.shape\", resized_frames.shape)\n",
        "    z_where, _ = self.encode_where(resized_frames)\n",
        "    worm_frames = crop_frames(frames, z_where)\n",
        "    print(\"worm_frames.shape\", worm_frames.shape)\n",
        "\n",
        "    # print(\"z_what_tile.shape\", z_what_tile.shape)\n",
        "    worm_frames_flattened = worm_frames.reshape(worm_frames.shape[:-2] + (-1,)) \n",
        "    proposed_sim_params = self.encode_sim(worm_frames_flattened)\n",
        "    worm_sim = numpyro.handlers.condition(sim_worms, {'L': proposed_sim_params[0], 'A': proposed_sim_params[2], 'T': proposed_sim_params[4], 'kw': proposed_sim_params[6], 'ku': proposed_sim_params[8], 'inc': proposed_sim_params[10], 'dr': proposed_sim_params[12], 'phase_1': proposed_sim_params[14], 'phase_2': proposed_sim_params[16], 'phase_3': proposed_sim_params[18], 'alpha': proposed_sim_params[20]})\n",
        "    worm_trace = numpyro.handlers.trace(worm_sim).get_trace(2, T)\n",
        "    worms = worm_trace[\"worms\"][\"value\"]\n",
        "\n",
        "    # print(\"L shape\", proposed_sim_params[0].shape)\n",
        "\n",
        "    # # z_what is normally sampled from a Gaussian with mean worms, \n",
        "    # # but doesn't matter for the purpose of initializing NNs\n",
        "\n",
        "    # print(\"reshape_z_what.shape\", reshape_z_what.shape)\n",
        "    worm_frame_recon = self.decode_what(worms)\n",
        "    # worm_frame_recon = self.decode_what(z_what)\n",
        "    frames_recon = embed_frames(worm_frame_recon, z_where, self.frame_size)\n",
        "    # print(\"frames_recon.shape\", frames_recon.shape)\n",
        "    return frames_recon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we define the target and kernels as in Section 6.4.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def wormsim_target(network, inputs, D=2, T=10, sleep_phase = False):\n",
        "\n",
        "  worms, params = sim_worms(D, T)\n",
        "\n",
        "  # print(\"L target\", params['L'][0, 0])\n",
        "\n",
        "  z_where = []\n",
        "  for d in range(D):\n",
        "    z_where_d = []\n",
        "    z_where_d_t = jnp.zeros(2)\n",
        "    for t in range(T):\n",
        "      scale = 1 if t == 0 else 0.05\n",
        "      z_where_d_t = numpyro.sample(\n",
        "          f\"z_where_{d}_{t}\", dist.Normal(z_where_d_t, scale).to_event(1)\n",
        "          # f\"z_where_{d}_{t}\", dist.TruncatedNormal(z_where_d_t, scale, low=-1.2, high=1.2).to_event(1)\n",
        "      )\n",
        "      # print(\"z_where_d_t target\", z_where_d_t[0, 0, 0])\n",
        "      z_where_d.append(z_where_d_t)\n",
        "    z_where_d = jnp.stack(z_where_d, -2)\n",
        "    z_where.append(z_where_d)\n",
        "  z_where = jnp.stack(z_where, -3)\n",
        "\n",
        "  worm_frames = jax.lax.stop_gradient(network.decode_what(worms))\n",
        "  \n",
        "  \n",
        "  # z_where = np.asarray(z_where).copy()\n",
        "  # z_where[0, 0, 0, 0] = np.ones((2,))\n",
        "  # worm_frames = network.decode_what(z_what)\n",
        "  # print(\"worm_frames.shape target\", worm_frames.shape)\n",
        "\n",
        "  # print(\"z_where target\", z_where.shape)\n",
        "  p = embed_frames(worm_frames, z_where, network.frame_size)\n",
        "  # print(\"p.shape target\", p.shape)\n",
        "  p = dist.util.clamp_probs(p.sum(-4))  # sum across worm_frames\n",
        "  # print(\"summed p.shape target\", p.shape)\n",
        "  # print(\"inputs.shape\", inputs.shape)\n",
        "  if sleep_phase:\n",
        "    frames = numpyro.sample(\"frames\", dist.Bernoulli(p).to_event(3))\n",
        "  else:\n",
        "    frames = numpyro.sample(\"frames\", dist.Bernoulli(p).to_event(3), obs=inputs)\n",
        "\n",
        "  # print(z_where.shape)\n",
        "  # print(z_where[0, 0, 0, 0])\n",
        "  # print(z_where[0, 0, 1, 0])\n",
        "  # print(\"worm_frames.shape target\", worm_frames.shape)\n",
        "  # plt.figure()\n",
        "  # plt.imshow(p[0, 0, 0, :, :])\n",
        "  # plt.colorbar()\n",
        "  # plt.figure()\n",
        "  # plt.imshow(worm_frames[0, 0, 0, 0, :, :])\n",
        "  # plt.colorbar()\n",
        "  # plt.figure()\n",
        "  # plt.imshow(worm_frames[0, 0, 1, 0, :, :])\n",
        "  # plt.colorbar()\n",
        "\n",
        "  # worm_frames_2 = crop_frames(p, -2 - z_where, 28)\n",
        "  # # worm_frames_2 = crop_frames(p[0, 0, 0], z_where[0, 0, 0, 0][..., ::-1] - 2, 28)\n",
        "  # # worm_frames_2 = crop_frames(p[0, 0, 0], -2 - z_where[0, 0, 0, 0], 28)\n",
        "  # # worm_frames_5 = crop_frames(p[0, 0, 0], z_where[0, 0, 1, 0][..., ::-1] - 2, 28)\n",
        "  # # worm_frames_5 = crop_frames(p[0, 0, 0], jnp.array([-1.1, -0.5]), 28)\n",
        "  # print(\"worm_frames_2.shape target\", worm_frames_2.shape)  \n",
        "  # plt.figure()\n",
        "  # plt.imshow(worm_frames_2[0, 0, 0, 0, :, :])\n",
        "  # plt.colorbar()\n",
        "  # plt.figure()\n",
        "  # plt.imshow(worm_frames_2[0, 0, 1, 0, :, :])\n",
        "  # plt.colorbar()\n",
        "\n",
        "  out = {\n",
        "      \"frames\": frames,\n",
        "      # \"frames_recon\": p,\n",
        "      \"frames_recon\": jax.lax.stop_gradient(p),\n",
        "      \"worms\": worms,\n",
        "      \"worm_frames\": jax.lax.stop_gradient(worm_frames),\n",
        "      \"params\": params,\n",
        "      **{f\"z_where_{t}\": z_where[..., t, :] for t in range(T)},\n",
        "  }\n",
        "  return (out,)\n",
        "\n",
        "\n",
        "def kernel_where(network, inputs, D=2, t=0, T=10, sleep_phase = False):\n",
        "  if not isinstance(inputs, dict):\n",
        "    # print('making inputs')\n",
        "    inputs = {\n",
        "        \"frames\": inputs,\n",
        "        \"worm_frames\": jnp.ones((D, T, 28, 28)),\n",
        "    }\n",
        "  # print(\"kernel where sleep phase\", sleep_phase)\n",
        "\n",
        "  if sleep_phase:\n",
        "    frame = inputs[\"frames_recon\"][..., t, :, :]\n",
        "  else:\n",
        "    frame = inputs[\"frames\"][..., t, :, :]\n",
        "  z_where_t = []\n",
        "\n",
        "  for d in range(D):\n",
        "    # print(inputs[\"worm_frames\"].shape)\n",
        "    worm_frame = inputs[\"worm_frames\"][..., d, t, :, :]\n",
        "    # print(\"worm_frame shape where\", worm_frame.shape)\n",
        "    resized_frame = resize_batch(frame, 32)\n",
        "    loc, scale = network.encode_where(resized_frame)\n",
        "    # print(loc.shape)\n",
        "    if sleep_phase:\n",
        "      z_where_d_t = numpyro.sample(\n",
        "          f\"z_where_{d}_{t}\", dist.Normal(loc, scale).to_event(1),\n",
        "          obs=inputs[f\"z_where_{d}_{t}\"]\n",
        "      )\n",
        "      # print(\"z_where_d_t where\", z_where_d_t[0, 0, 0])\n",
        "    else:\n",
        "      z_where_d_t = numpyro.sample(\n",
        "          f\"z_where_{d}_{t}\", dist.Normal(loc, scale).to_event(1)\n",
        "      )\n",
        "    z_where_t.append(z_where_d_t)\n",
        "    # print(\"worm_frame shape where\", worm_frame.shape)\n",
        "    # print(\"z_where_d_t shape where\", z_where_d_t.shape)\n",
        "    frame_recon = embed_frames(worm_frame, z_where_d_t, network.frame_size)\n",
        "    # print(\"frame_recon shape where\", frame_recon)\n",
        "    frame = frame - frame_recon\n",
        "  z_where_t = jnp.stack(z_where_t, -2)\n",
        "  # print(\"z_where_t.shape where\", z_where_t.shape)\n",
        "  out = {**inputs, **{f\"z_where_{t}\": z_where_t}}\n",
        "  return (out,)\n",
        "\n",
        "\n",
        "def kernel_sim(network, inputs, T=10, sleep_phase = False):\n",
        "  # print(\"kernel sim sleep phase\", sleep_phase)\n",
        "  z_where = jnp.stack([inputs[f\"z_where_{t}\"] for t in range(T)], -2)\n",
        "  if sleep_phase:\n",
        "    worm_frames = crop_frames(inputs[\"frames_recon\"], -2 - z_where, 28)\n",
        "  else:\n",
        "    worm_frames = crop_frames(inputs[\"frames\"], -2 - z_where, 28)\n",
        "\n",
        "  # print(z_where.shape)\n",
        "  # print(z_where[0, 0, 0, 0] * 32 + 32)\n",
        "  # print(z_where[0, 0, 1, 0] * 32 + 32)\n",
        "  # print(\"worm_frames.shape sim\", worm_frames.shape)\n",
        "  # plt.figure()\n",
        "  # plt.imshow(inputs[\"frames_recon\"][0, 0, 0, :, :])\n",
        "  # plt.figure()\n",
        "  # plt.imshow(worm_frames[0, 0, 0, 0, :, :])\n",
        "  # plt.figure()\n",
        "  # plt.imshow(worm_frames[0, 0, 1, 0, :, :])\n",
        "  # plt.figure()\n",
        "  # plt.imshow(inputs[\"worm_frames\"][0, 0, 0, 0, :, :])\n",
        "  # plt.figure()\n",
        "  # plt.imshow(inputs[\"worm_frames\"][0, 0, 1, 0, :, :])\n",
        "  worm_frames_flattened = worm_frames.reshape(worm_frames.shape[:-2] + (-1,)) \n",
        "  proposed_sim_params = network.encode_sim(worm_frames_flattened)\n",
        "  loc_L, scale_L, loc_A, scale_A, loc_T, scale_T, loc_kw, scale_kw, loc_ku, scale_ku, loc_inc, scale_inc, loc_dr, scale_dr, loc_phase_1, scale_phase_1, loc_phase_2, scale_phase_2, loc_phase_3, scale_phase_3, loc_alpha, scale_alpha = proposed_sim_params\n",
        "\n",
        "  if sleep_phase:\n",
        "    L = numpyro.sample(\"L\", dist.Normal(loc_L, scale_L).to_event(1), obs=inputs[\"L\"])\n",
        "    # print(\"L sim\", L[0, 0])\n",
        "    A = numpyro.sample(\"A\", dist.Normal(loc_A, scale_A).to_event(1), obs=inputs[\"A\"])\n",
        "    T = numpyro.sample(\"T\", dist.Normal(loc_T, scale_T).to_event(1), obs=inputs[\"T\"])\n",
        "    kw = numpyro.sample(\"kw\", dist.Normal(loc_kw, scale_kw).to_event(1), obs=inputs[\"kw\"])\n",
        "    ku = numpyro.sample(\"ku\", dist.Normal(loc_ku, scale_ku).to_event(1), obs=inputs[\"ku\"])\n",
        "    inc = numpyro.sample(\"inc\", dist.Normal(loc_inc, scale_inc).to_event(1), obs=inputs[\"inc\"])\n",
        "    dr = numpyro.sample(\"dr\", dist.Normal(loc_dr, scale_dr).to_event(1), obs=inputs[\"dr\"])\n",
        "    phase_1 = numpyro.sample(\"phase_1\", dist.Normal(loc_phase_1, scale_phase_1).to_event(1), obs=inputs[\"phase_1\"])\n",
        "    phase_2 = numpyro.sample(\"phase_2\", dist.Normal(loc_phase_2, scale_phase_2).to_event(1), obs=inputs[\"phase_2\"])\n",
        "    phase_3 = numpyro.sample(\"phase_3\", dist.Normal(loc_phase_3, scale_phase_3).to_event(1), obs=inputs[\"phase_3\"])\n",
        "    alpha = numpyro.sample(\"alpha\", dist.Normal(loc_alpha, scale_alpha).to_event(1), obs=inputs[\"alpha\"])\n",
        "  else:\n",
        "    L = numpyro.sample('L', dist.TruncatedNormal(loc_L, scale_L, low=23, high=28).to_event(1))\n",
        "    A = numpyro.sample('A', dist.Normal(loc_A, scale_A).to_event(1))\n",
        "    T = numpyro.sample('T', dist.Normal(loc_T, scale_T).to_event(1))\n",
        "    kw = numpyro.sample('kw', dist.TruncatedNormal(loc_kw, scale_kw, low=0, high=2 * jnp.pi).to_event(1))\n",
        "    ku = numpyro.sample('ku', dist.Normal(loc_ku, scale_ku).to_event(1))\n",
        "    inc = numpyro.sample('inc', dist.TruncatedNormal(loc_inc, scale_inc, low=0, high=2 * jnp.pi).to_event(1))\n",
        "    dr = numpyro.sample('dr', dist.TruncatedNormal(loc_dr, scale_dr, low=0.2, high=0.8).to_event(1))\n",
        "    phase_1 = numpyro.sample('phase_1', dist.TruncatedNormal(loc_phase_1, scale_phase_1, low=0, high=2 * jnp.pi).to_event(1))\n",
        "    phase_2 = numpyro.sample('phase_2', dist.TruncatedNormal(loc_phase_2, scale_phase_2, low=0, high=2 * jnp.pi).to_event(1))\n",
        "    phase_3 = numpyro.sample('phase_3', dist.Normal(loc_phase_3, scale_phase_3).to_event(1))\n",
        "    alpha = numpyro.sample('alpha', dist.Normal(loc_alpha, scale_alpha).to_event(1))\n",
        "\n",
        "  out = {**inputs, **{'L': L, 'A': A, 'T': T, 'kw': kw, 'ku': ku, 'inc': inc, 'dr': dr, 'phase_1': phase_1, 'phase_2': phase_2, 'phase_3': phase_3, 'alpha': alpha}}\n",
        "  return (out,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we create the inference program, define the loss function,\n",
        "run the training loop, and plot the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Args(argparse.Namespace):\n",
        "  batch_size = 14\n",
        "  # batch_size = 16\n",
        "  num_sweeps = 5\n",
        "  num_particles = 1\n",
        "  # learning_rate = 3e-4\n",
        "  # learning_rate = 1e-4\n",
        "  learning_rate = 1e-5\n",
        "  # num_steps = 600_000\n",
        "  # num_steps = 10\n",
        "  num_steps = 40_000\n",
        "  # num_steps = 4000\n",
        "  # num_steps = 2000\n",
        "  device = \"gpu\"\n",
        "\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1724096966.032729   32596 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-08-19 21:49:26.064362: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2024-08-19 21:49:26.351119: W external/xla/xla/service/gpu/nvptx_compiler.cc:836] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version (12.5.82). Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "worm_frames.shape (14, 10, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "lr = args.learning_rate\n",
        "num_steps = args.num_steps\n",
        "batch_size = args.batch_size\n",
        "num_sweeps = args.num_sweeps\n",
        "num_particles = args.num_particles\n",
        "\n",
        "train_ds = load_dataset(is_training=True, batch_size=batch_size, n_data=-1)\n",
        "\n",
        "test_ds = load_dataset(is_training=False, batch_size=batch_size)\n",
        "test_data = next(test_ds)\n",
        "frame_size = test_data.shape[-1]\n",
        "wormsim_net = wormsimAutoEncoder(num_particles=num_particles, batch_size=batch_size, frame_size=frame_size)\n",
        "init_params = wormsim_net.init(jax.random.PRNGKey(0), test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_proposal(network, make_particle_plate, T=10, sleep_phase=False):\n",
        "  kernels = []\n",
        "  for t in range(T):\n",
        "    kernels.append(\n",
        "        make_particle_plate()(partial(kernel_where, network, D=2, t=t, sleep_phase=sleep_phase))\n",
        "    )\n",
        "  kernels.append(make_particle_plate()(partial(kernel_sim, network, T=T, sleep_phase=sleep_phase)))\n",
        "\n",
        "  kernels = [detach(k) for k in kernels]\n",
        "  q = reduce(lambda a, b: compose(b, a), kernels[1:], kernels[0])\n",
        "  return q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def loss_fn(params, key, batch, wormsim_net, num_particles):\n",
        "def loss_fn(params, key, wormsim_net, num_particles, batch_size):\n",
        "    D = 2\n",
        "    T = 10\n",
        "    network = coix.util.BindModule(wormsim_net, params)\n",
        "    make_particle_plate = lambda: numpyro.plate(\"particle\", num_particles, dim=-2)\n",
        "    make_batch_plate = lambda: numpyro.plate(\"batch\", batch_size, dim=-1)\n",
        "    target = make_particle_plate()(make_batch_plate()(partial(wormsim_target, network, D=D, T=T)))\n",
        "\n",
        "    shuffle_rng, rng_key = random.split(key)\n",
        "    # sample from the target\n",
        "    out_model, tr_model, _ = coix.traced_evaluate(target, seed=jax.random.PRNGKey(rng_key[0]))(\n",
        "        None, sleep_phase=True\n",
        "    )\n",
        "    model_sample = {k: v[\"value\"] for k, v in tr_model.items()}\n",
        "\n",
        "    proposal_io = {**out_model[0], **model_sample}\n",
        "\n",
        "    q = make_proposal(network, make_particle_plate, T=T, sleep_phase=True)\n",
        "\n",
        "    out_q, tr_q, metrics = coix.traced_evaluate(q, seed=jax.random.PRNGKey(rng_key[1]))(proposal_io, sleep_phase=True)\n",
        "    q_log_probs = {\n",
        "        name: util.get_site_log_prob(site) for name, site in tr_q.items()\n",
        "    }\n",
        "    # print({k: v[\"is_observed\"] for k, v in tr_q.items()})\n",
        "    # print(model_sample['z_what_0'][0, 0, 0])\n",
        "    # print(tr_q['z_what_0']['value'][0, 0, 0])\n",
        "    # print(out_model[0]['worm_frames'].shape)\n",
        "    # plt.figure()\n",
        "    # plt.imshow(out_model[0]['worm_frames'][0, 0, 0, 0])\n",
        "    # plt.figure()\n",
        "    # plt.imshow(out_model[0]['frames_recon'][0, 0, 0])\n",
        "\n",
        "    # print(model_sample[f\"z_what_{0}\"][0, 0, 0][:5])\n",
        "    # print(tr_q[f\"z_what_{0}\"]['value'][0, 0, 0][:5])\n",
        "    q_log_density = sum(q_log_probs.values()).mean()\n",
        "\n",
        "    z_where_loss = sum(\n",
        "        q_log_probs[f\"z_where_{d}_{t}\"]\n",
        "        for d in range(D)\n",
        "        for t in range(T)\n",
        "    ).mean()\n",
        "    sim_loss = q_log_probs[\"L\"] + q_log_probs[\"A\"] + q_log_probs[\"T\"] + q_log_probs[\"kw\"] + q_log_probs[\"ku\"] + q_log_probs[\"inc\"] + q_log_probs[\"dr\"] + q_log_probs[\"phase_1\"] + q_log_probs[\"phase_2\"] + q_log_probs[\"phase_3\"] + q_log_probs[\"alpha\"]\n",
        "    metrics['z_where_loss'] = -z_where_loss\n",
        "    metrics['sim_loss'] = -sim_loss.mean()\n",
        "    metrics['loss'] = -q_log_density\n",
        "    return -q_log_density, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# foo, bar = loss_fn(init_params, jax.random.PRNGKey(0), wormsim_net, num_particles, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr_schedule = cosine_decay_schedule(lr, num_steps, 0.5)\n",
        "# lr_schedule = cosine_decay_schedule(lr, num_steps, 1.0)\n",
        "\n",
        "opt = optax.chain(\n",
        "    clip_by_global_norm(1.0),\n",
        "    optax.adam(lr_schedule),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# wormsim_params = np.load(\"worm_learned_params_2.npy\", allow_pickle=True).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# eval_fn(step, params, opt_state, metrics)\n",
        "\n",
        "def eval_fn(step, params, opt_state, metrics):\n",
        "    # print all params norms using tree map\n",
        "    pprint(jax.tree.map(lambda x: jnp.linalg.norm(x), params))\n",
        "    if 'param_norms' in metrics:\n",
        "        metrics['param_norms'].append(jax.tree_map(lambda x: jnp.linalg.norm(x), params))\n",
        "    else:\n",
        "        metrics['params_norms'] = []\n",
        "    # print all metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compiling the first train step...\n",
            "Time to compile a train step: 34.50862002372742\n",
            "=====\n",
            "Step 500   | loss    88.4127 | sim_loss    41.0611 | squared_grad_norm  9805.9980 | z_where_loss    47.3516\n",
            "Step 1000  | loss    92.1822 | sim_loss    35.7317 | squared_grad_norm 19933.6523 | z_where_loss    56.4505\n",
            "Step 1500  | loss    64.9610 | sim_loss    31.5297 | squared_grad_norm  5896.1733 | z_where_loss    33.4313\n",
            "Step 2000  | loss    78.4456 | sim_loss    28.3188 | squared_grad_norm  3964.0791 | z_where_loss    50.1268\n",
            "Step 2500  | loss    75.7222 | sim_loss    27.1485 | squared_grad_norm 13201.1309 | z_where_loss    48.5737\n",
            "Step 3000  | loss    66.6975 | sim_loss    24.8202 | squared_grad_norm  2071.4312 | z_where_loss    41.8773\n",
            "Step 3500  | loss    66.8219 | sim_loss    24.9891 | squared_grad_norm  3914.8469 | z_where_loss    41.8328\n",
            "Step 4000  | loss    79.4034 | sim_loss    31.2903 | squared_grad_norm 63467.2188 | z_where_loss    48.1132\n",
            "Step 4500  | loss    67.9503 | sim_loss    24.0239 | squared_grad_norm  5187.1777 | z_where_loss    43.9264\n",
            "Step 5000  | loss    66.2003 | sim_loss    22.2472 | squared_grad_norm  5589.5420 | z_where_loss    43.9531\n",
            "Step 5500  | loss    77.4549 | sim_loss    23.9140 | squared_grad_norm 10996.6602 | z_where_loss    53.5409\n",
            "Step 6000  | loss    67.4333 | sim_loss    23.1955 | squared_grad_norm 17253.7832 | z_where_loss    44.2378\n",
            "Step 6500  | loss    59.6350 | sim_loss    22.5113 | squared_grad_norm  7916.8423 | z_where_loss    37.1238\n",
            "Step 7000  | loss    71.0905 | sim_loss    21.5072 | squared_grad_norm  6398.5581 | z_where_loss    49.5834\n",
            "Step 7500  | loss    63.4493 | sim_loss    23.0726 | squared_grad_norm 25185.2812 | z_where_loss    40.3766\n",
            "Step 8000  | loss    70.1750 | sim_loss    23.1972 | squared_grad_norm 13786.1807 | z_where_loss    46.9779\n",
            "Step 8500  | loss    70.1542 | sim_loss    21.7925 | squared_grad_norm 10217.9727 | z_where_loss    48.3617\n",
            "Step 9000  | loss    69.9381 | sim_loss    22.7431 | squared_grad_norm 10217.2617 | z_where_loss    47.1950\n",
            "Step 9500  | loss    63.2281 | sim_loss    21.7704 | squared_grad_norm  6466.9629 | z_where_loss    41.4578\n",
            "Step 10000 | loss    64.5321 | sim_loss    22.0575 | squared_grad_norm  8962.8955 | z_where_loss    42.4746\n",
            "Step 10500 | loss    56.8451 | sim_loss    21.9907 | squared_grad_norm  3415.1948 | z_where_loss    34.8544\n",
            "Step 11000 | loss    60.0105 | sim_loss    22.4980 | squared_grad_norm 15271.4844 | z_where_loss    37.5124\n",
            "Step 11500 | loss    61.0091 | sim_loss    21.7675 | squared_grad_norm  6141.1831 | z_where_loss    39.2416\n",
            "Step 12000 | loss    62.0704 | sim_loss    22.6500 | squared_grad_norm  9602.5166 | z_where_loss    39.4204\n",
            "Step 12500 | loss    51.6721 | sim_loss    21.6927 | squared_grad_norm  7191.6084 | z_where_loss    29.9794\n",
            "Step 13000 | loss    73.1220 | sim_loss    22.4136 | squared_grad_norm 24606.4375 | z_where_loss    50.7084\n",
            "Step 13500 | loss    61.8949 | sim_loss    22.0707 | squared_grad_norm  5888.5708 | z_where_loss    39.8242\n",
            "Step 14000 | loss    62.7742 | sim_loss    22.4872 | squared_grad_norm  9088.6592 | z_where_loss    40.2870\n",
            "Step 14500 | loss    71.8069 | sim_loss    22.5068 | squared_grad_norm 14856.5596 | z_where_loss    49.3001\n",
            "Step 15000 | loss    55.4669 | sim_loss    20.9549 | squared_grad_norm  5466.9722 | z_where_loss    34.5119\n",
            "Step 15500 | loss    61.5007 | sim_loss    22.3859 | squared_grad_norm  7256.9097 | z_where_loss    39.1148\n",
            "Step 16000 | loss    71.7409 | sim_loss    20.9331 | squared_grad_norm 26474.1621 | z_where_loss    50.8078\n",
            "Step 16500 | loss    58.1585 | sim_loss    20.6640 | squared_grad_norm  7957.5625 | z_where_loss    37.4945\n",
            "Step 17000 | loss    65.5316 | sim_loss    20.4770 | squared_grad_norm 12286.2002 | z_where_loss    45.0547\n",
            "Step 17500 | loss    49.5457 | sim_loss    19.9376 | squared_grad_norm 12791.3369 | z_where_loss    29.6081\n",
            "Step 18000 | loss    63.7845 | sim_loss    22.2770 | squared_grad_norm 19006.3809 | z_where_loss    41.5075\n",
            "Step 18500 | loss    55.4151 | sim_loss    22.0757 | squared_grad_norm 11936.4336 | z_where_loss    33.3395\n",
            "Step 19000 | loss    64.3980 | sim_loss    21.9677 | squared_grad_norm  4422.8682 | z_where_loss    42.4303\n",
            "Step 19500 | loss    65.5147 | sim_loss    22.2082 | squared_grad_norm  7484.4888 | z_where_loss    43.3066\n",
            "Step 20000 | loss    63.7102 | sim_loss    20.8796 | squared_grad_norm  7999.5430 | z_where_loss    42.8306\n",
            "Step 20500 | loss    60.7256 | sim_loss    21.3047 | squared_grad_norm  6042.6641 | z_where_loss    39.4210\n",
            "Step 21000 | loss    60.3935 | sim_loss    21.7390 | squared_grad_norm  7602.2466 | z_where_loss    38.6545\n",
            "Step 21500 | loss    59.1049 | sim_loss    21.8951 | squared_grad_norm  6694.8682 | z_where_loss    37.2098\n",
            "Step 22000 | loss    56.0284 | sim_loss    20.8382 | squared_grad_norm  7250.5894 | z_where_loss    35.1903\n",
            "Step 22500 | loss    68.8951 | sim_loss    21.2815 | squared_grad_norm 13590.4121 | z_where_loss    47.6136\n",
            "Step 23000 | loss    70.7748 | sim_loss    21.4730 | squared_grad_norm 31080.1309 | z_where_loss    49.3018\n",
            "Step 23500 | loss    59.4718 | sim_loss    20.0962 | squared_grad_norm 12973.4033 | z_where_loss    39.3756\n",
            "Step 24000 | loss    58.3577 | sim_loss    19.8408 | squared_grad_norm  6822.1353 | z_where_loss    38.5170\n",
            "Step 24500 | loss    59.1574 | sim_loss    21.1927 | squared_grad_norm  4466.6270 | z_where_loss    37.9647\n",
            "Step 25000 | loss    47.3567 | sim_loss    19.9799 | squared_grad_norm 11321.8467 | z_where_loss    27.3768\n",
            "Step 25500 | loss    61.6164 | sim_loss    20.6078 | squared_grad_norm  6619.5684 | z_where_loss    41.0086\n",
            "Step 26000 | loss    69.4905 | sim_loss    22.3926 | squared_grad_norm 19616.8359 | z_where_loss    47.0979\n",
            "Step 26500 | loss    67.7938 | sim_loss    19.7309 | squared_grad_norm 20702.0391 | z_where_loss    48.0629\n",
            "Step 27000 | loss    52.4492 | sim_loss    21.9427 | squared_grad_norm 10352.5654 | z_where_loss    30.5064\n",
            "Step 27500 | loss    55.9676 | sim_loss    21.6090 | squared_grad_norm  6928.2891 | z_where_loss    34.3585\n",
            "Step 28000 | loss    78.1466 | sim_loss    20.3889 | squared_grad_norm 18087.4082 | z_where_loss    57.7577\n",
            "Step 28500 | loss    54.1543 | sim_loss    21.0235 | squared_grad_norm  8704.2852 | z_where_loss    33.1308\n",
            "Step 29000 | loss    73.6614 | sim_loss    21.2561 | squared_grad_norm 19803.8984 | z_where_loss    52.4053\n",
            "Step 29500 | loss    57.1456 | sim_loss    20.3957 | squared_grad_norm 13160.6523 | z_where_loss    36.7500\n",
            "Step 30000 | loss    59.6903 | sim_loss    20.8147 | squared_grad_norm  7821.0244 | z_where_loss    38.8757\n",
            "Step 30500 | loss    62.0518 | sim_loss    19.4977 | squared_grad_norm 12525.3633 | z_where_loss    42.5540\n",
            "Step 31000 | loss    57.4226 | sim_loss    22.4959 | squared_grad_norm 21114.4492 | z_where_loss    34.9267\n",
            "Step 31500 | loss    64.7923 | sim_loss    19.7461 | squared_grad_norm  5550.0308 | z_where_loss    45.0461\n",
            "Step 32000 | loss    73.8040 | sim_loss    19.9433 | squared_grad_norm 20581.9766 | z_where_loss    53.8607\n",
            "Step 32500 | loss    71.1962 | sim_loss    19.9110 | squared_grad_norm 40735.8438 | z_where_loss    51.2853\n",
            "Step 33000 | loss    55.0201 | sim_loss    20.5474 | squared_grad_norm 12965.3223 | z_where_loss    34.4727\n",
            "Step 33500 | loss    72.8272 | sim_loss    21.6436 | squared_grad_norm 40196.5312 | z_where_loss    51.1836\n",
            "Step 34000 | loss    67.9006 | sim_loss    20.8450 | squared_grad_norm 21216.9668 | z_where_loss    47.0556\n",
            "Step 34500 | loss    68.7976 | sim_loss    20.3788 | squared_grad_norm 20228.5156 | z_where_loss    48.4188\n",
            "Step 35000 | loss    52.5631 | sim_loss    21.7297 | squared_grad_norm 25801.0664 | z_where_loss    30.8334\n",
            "Step 35500 | loss    62.9954 | sim_loss    21.4388 | squared_grad_norm  9501.0273 | z_where_loss    41.5566\n",
            "Step 36000 | loss    66.2273 | sim_loss    19.9943 | squared_grad_norm 19634.6289 | z_where_loss    46.2330\n",
            "Step 36500 | loss    50.7619 | sim_loss    20.3073 | squared_grad_norm 10458.5928 | z_where_loss    30.4545\n",
            "Step 37000 | loss    86.8232 | sim_loss    20.9177 | squared_grad_norm 33668.8867 | z_where_loss    65.9055\n",
            "Step 37500 | loss    55.6085 | sim_loss    21.0232 | squared_grad_norm  6279.5996 | z_where_loss    34.5854\n",
            "Step 38000 | loss    47.5944 | sim_loss    21.2024 | squared_grad_norm 11696.3291 | z_where_loss    26.3920\n",
            "Step 38500 | loss    59.5331 | sim_loss    21.6255 | squared_grad_norm 18169.5879 | z_where_loss    37.9076\n",
            "Step 39000 | loss    59.2654 | sim_loss    19.4594 | squared_grad_norm  8662.8096 | z_where_loss    39.8059\n",
            "Step 39500 | loss    71.7002 | sim_loss    21.7706 | squared_grad_norm 14822.8125 | z_where_loss    49.9296\n",
            "Step 40000 | loss    66.8725 | sim_loss    20.9374 | squared_grad_norm 15755.6318 | z_where_loss    45.9351\n"
          ]
        }
      ],
      "source": [
        "# with jax.disable_jit():\n",
        "#     wormsim_params, metrics = coix.util.train(\n",
        "#         partial(\n",
        "#             loss_fn,\n",
        "#             wormsim_net=wormsim_net,\n",
        "#             num_particles=num_particles,\n",
        "#             batch_size=batch_size,\n",
        "#         ),\n",
        "#         init_params,\n",
        "#         # wormsim_params,\n",
        "#         # optax.adam(lr),\n",
        "#         opt,\n",
        "#         num_steps,\n",
        "#         # train_ds,\n",
        "#         # eval_fn=eval_fn,\n",
        "#         # log_every=500,\n",
        "#         log_every=1,\n",
        "#     )\n",
        "wormsim_params, metrics = coix.util.train(\n",
        "    partial(\n",
        "        loss_fn,\n",
        "        wormsim_net=wormsim_net,\n",
        "        num_particles=num_particles,\n",
        "        batch_size=batch_size,\n",
        "    ),\n",
        "    init_params,\n",
        "    # wormsim_params,\n",
        "    # optax.adam(lr),\n",
        "    opt,\n",
        "    num_steps,\n",
        "    # train_ds,\n",
        "    # eval_fn=eval_fn,\n",
        "    log_every=500,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'log_weight': Array([[ -51.858772,  -80.67902 ,  -79.48616 ,  -75.734406,  -67.32074 ,\n",
              "          -42.09323 ,  -60.349464,  -48.384052,  -32.668777,  -73.91752 ,\n",
              "          -41.142498,  -87.31272 ,  -71.42123 , -123.84654 ]],      dtype=float32),\n",
              " 'loss': Array(66.87252, dtype=float32),\n",
              " 'sim_loss': Array(20.93737, dtype=float32),\n",
              " 'squared_grad_norm': Array(15755.632, dtype=float32),\n",
              " 'z_where_loss': Array(45.935143, dtype=float32)}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.save(\"worm_sleep_phase_learned_params.npy\", wormsim_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "T = 10\n",
        "D = 2\n",
        "network = coix.util.BindModule(wormsim_net, wormsim_params)\n",
        "make_particle_plate = lambda: numpyro.plate(\"particle\", num_particles, dim=-2)\n",
        "make_batch_plate = lambda: numpyro.plate(\"batch\", batch_size, dim=-1)\n",
        "target = make_particle_plate()(make_batch_plate()(partial(wormsim_target, network, D=D, T=T)))\n",
        "shuffle_rng, rng_key = random.split(jax.random.PRNGKey(0))\n",
        "# sample from the target\n",
        "out_model, tr_model, _ = coix.traced_evaluate(target, seed=jax.random.PRNGKey(rng_key[0]))(\n",
        "    None, sleep_phase=False\n",
        ")\n",
        "model_sample = {k: v[\"value\"] for k, v in tr_model.items()}\n",
        "proposal_io = {**out_model[0], **model_sample}\n",
        "q = make_proposal(network, make_particle_plate, T=T)\n",
        "# out_q, tr_q, metrics = coix.traced_evaluate(q, seed=jax.random.PRNGKey(rng_key[1]))(proposal_io, sleep_phase=False)\n",
        "out_q, tr_q, metrics = coix.traced_evaluate(propose(target, q), seed=jax.random.PRNGKey(rng_key[1]))(out_model[0]['frames_recon'], sleep_phase=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['L', 'A', 'T', 'kw', 'ku', 'inc', 'dr', 'phase_1', 'phase_2', 'phase_3', 'alpha', 'z_where_0_0', 'z_where_0_1', 'z_where_0_2', 'z_where_0_3', 'z_where_0_4', 'z_where_0_5', 'z_where_0_6', 'z_where_0_7', 'z_where_0_8', 'z_where_0_9', 'z_where_1_0', 'z_where_1_1', 'z_where_1_2', 'z_where_1_3', 'z_where_1_4', 'z_where_1_5', 'z_where_1_6', 'z_where_1_7', 'z_where_1_8', 'z_where_1_9', 'frames'])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tr_q.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 14, 10, 64, 64)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out_model[0]['frames_recon'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb58732ad50>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(out_model[0]['frames_recon'][0, 0, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['frames', 'frames_recon', 'worms', 'worm_frames', 'params', 'z_where_0', 'z_where_1', 'z_where_2', 'z_where_3', 'z_where_4', 'z_where_5', 'z_where_6', 'z_where_7', 'z_where_8', 'z_where_9'])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out_q[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb58734fce0>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(out_q[0]['frames_recon'][0, 0, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['L', 'A', 'T', 'kw', 'ku', 'inc', 'dr', 'phase_1', 'phase_2', 'phase_3', 'alpha', 'z_where_0_0', 'z_where_0_1', 'z_where_0_2', 'z_where_0_3', 'z_where_0_4', 'z_where_0_5', 'z_where_0_6', 'z_where_0_7', 'z_where_0_8', 'z_where_0_9', 'z_where_1_0', 'z_where_1_1', 'z_where_1_2', 'z_where_1_3', 'z_where_1_4', 'z_where_1_5', 'z_where_1_6', 'z_where_1_7', 'z_where_1_8', 'z_where_1_9', 'frames'])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tr_q.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
